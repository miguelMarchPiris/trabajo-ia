{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primero de todo los imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "\n",
    "# Creamos un directiorio para la salida\n",
    "import os\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "\n",
    "\n",
    "\n",
    "# Para hacer test y train\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Para ajustar los hiper-parámetros\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Preproceso\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Función de error\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to display score results from CV\n",
    "def display_scores(scores,model_name = None):\n",
    "    if(model_name):\n",
    "        print(\"----\",model_name,\"----\")\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de casas: 5432  Número de características: 21\n",
      "\n",
      " Características numéricas: \n",
      "  ['Rooms', 'Price', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
      "Características categóricas: \n",
      " ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el dataset\n",
    "housing = pd.read_csv('dataset/housing-snapshot/train_set.csv',index_col=0) \n",
    "print(\"Número de casas:\",housing.shape[0],\" Número de características:\", housing.shape[1])\n",
    "housing_num = housing.select_dtypes(exclude=[np.object]).columns\n",
    "housing_cat = housing.select_dtypes(include=[np.object]).columns\n",
    "print(\"\\n Características numéricas: \\n \", list(housing_num))\n",
    "\n",
    "print(\"Características categóricas: \\n\", list(housing_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ponemos \"Postcode\" a categorical, dado que que no es numérica.\n",
    "housing['Postcode'] = pd.Categorical(housing.Postcode)\n",
    "\n",
    "# Dividimos haciendo uso de la estratificación para tener una buena proporción.\n",
    "housing[\"price_aux\"] = pd.cut(housing[\"Price\"],\n",
    "                               bins=[0., 500000, 1000000, 1500000, 2000000., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"price_aux\"]):\n",
    "    train_set = housing.loc[train_index]\n",
    "    test_set = housing.loc[test_index]\n",
    "\n",
    "# Quitamos la etiqueta price_cat\n",
    "train_set.drop(\"price_aux\", axis=1, inplace=True)\n",
    "test_set.drop(\"price_aux\", axis=1, inplace=True)\n",
    "housing.drop(\"price_aux\", axis=1, inplace=True)\n",
    "\n",
    "# El con el train probaremos los modelos.\n",
    "X_train = train_set.drop(\"Price\", axis=1).copy()\n",
    "y_train = train_set[\"Price\"].copy()\n",
    "# El test no lo tenemos que tocar hasta el final de todo.\n",
    "X_test = test_set.drop(\"Price\", axis=1).copy()\n",
    "y_test = test_set[\"Price\"].copy()\n",
    "\n",
    "\n",
    "housing_num = X_train.select_dtypes(exclude=[np.object]).columns\n",
    "housing_cat = X_train.select_dtypes(include=[np.object]).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Las funciones de las pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a function to replace 0 by NaN\n",
    "def replace_0_2_NaN(data):\n",
    "    data[data == 0] = np.nan\n",
    "    return data\n",
    "\n",
    "\n",
    "# column index\n",
    "Rooms_ix, Bedroom2_ix, Bathroom_ix, BuildingArea_ix = 0, 2, 3, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        rooms_per_building_area = X[:, Rooms_ix] / (1.0 +X[:, BuildingArea_ix])# add 1 to avoid 0 division\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, Bedroom2_ix] / (1.0 + X[:, Bathroom_ix]) # add 1 to avoid 0 division\n",
    "            return np.c_[X, rooms_per_building_area, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_building_area]\n",
    "class Clean_Outlayers_Quantile(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,q=0.01): # no *args or **kargs\n",
    "        self.q=q\n",
    "        self.low_q_col=[]\n",
    "        self.high_q_col=[]\n",
    "        #self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        #Para cada columna\n",
    "        for i in range(X.shape[1]):\n",
    "            columna = X[:,i]\n",
    "            self.high_q_col.append(np.quantile(a=columna , q=1-self.q))\n",
    "            self.low_q_col.append(np.quantile(a=columna ,q=self.q))\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        for i in range(X.shape[1]):\n",
    "            q_high=self.high_q_col[i]\n",
    "            q_low =self.low_q_col[i]\n",
    "            columna = X[:,i]\n",
    "            columna[columna>q_high]=q_high\n",
    "            columna[columna<q_low]=q_low\n",
    "        return X\n",
    "\n",
    "# Ejemplo de PipeLines\n",
    "\"\"\"\n",
    "1.Para las columnas con muchos ceros sin sentido. \n",
    "Además se les aplicará la función logaritmo.\n",
    "Las columnas que tienen sentido en esta pipe son \"BuildingArea\" y \"Landsize\" \n",
    "\"\"\"\n",
    "num0_pipeline = Pipeline([\n",
    "        ('zeros2NaN',FunctionTransformer(func = replace_0_2_NaN,validate=False)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('log',FunctionTransformer(np.log1p, validate=True)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# 2.Para las otras columnas numéricas\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Para las variables categóricas. Principalmente utilizaremos el OneHotEncoder\n",
    "Esta pipe convierte columnas categóricas en numéricas creando una columna de \n",
    "1's y 0's por cada valor único en la columna original.\n",
    "\n",
    "Ejemplo:\n",
    "Original \"Type\":\n",
    "Type\n",
    "h\n",
    "t\n",
    "h\n",
    "u\n",
    "h\n",
    "\n",
    "Dummies from \"Type\":\n",
    "h t u\n",
    "1 0 0\n",
    "0 1 0\n",
    "1 0 0\n",
    "0 0 1\n",
    "1 0 0\n",
    "\n",
    "Esto sube muchísimo la dimensionalidad si la columna original\n",
    "tiene muchos valores distintos, así que hay que tratarlo con cuidado.\n",
    "\n",
    "\"\"\"\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"constant\",fill_value='Unknown')),\n",
    "        ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "num_attribs0 = ['Landsize','BuildingArea']\n",
    "num_attribs1 = list(housing_num)\n",
    "cat_attribs = [\"CouncilArea\",'Type','Suburb','Postcode']\n",
    "# Esta es una especie de Pipeline madre que coge otras pipelines y procesa\n",
    "# todo el data set. Cada pipeline se aplica a las columnas de la lista\n",
    "# num0_pipeline con las columnas en la lista num_attribs0\n",
    "full_pipeline_ejemplo = ColumnTransformer([\n",
    "        (\"num0\", num0_pipeline, num_attribs0),\n",
    "        (\"num1\", num_pipeline, num_attribs1),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los modelos\n",
    "\n",
    "Vamos a tratar unos modelos de machine learning y en concreto haremos uso de la famosa librería Sklearn. En cada uno de los distintos modelos vamos a mostrar primero una implementación naíf y luego vamos a hacer un preproceso de datos personalizado, una feature selection y cuando sea posible una optimización de los hiper-parámetros para conseguir los mejores resultados posibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn import neighbors    \n",
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación naíf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- lin_reg ----\n",
      "Mean: 384025.51472573285\n",
      "Standard deviation: 48252.21371717167\n",
      "---- KNN-Regressor ----\n",
      "Mean: 375914.6862240796\n",
      "Standard deviation: 31220.851831198084\n",
      "---- Random Forest ----\n",
      "Mean: 310410.0826121716\n",
      "Standard deviation: 36235.270271245245\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Para las otras columnas numéricas\n",
    "num_naif_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "#Para las variables categóricas. Principalmente utilizaremos el OneHotEncoder\n",
    "cat_naif_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"constant\",fill_value='Unknown')),\n",
    "        ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\"\"\"\n",
    "Columnas numéricas:  ['Rooms', 'Price', 'Distance', 'Bedroom2', 'Bathroom', 'Car', \n",
    "'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "\n",
    "Columnas categóricas:  ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
    "\"\"\"\n",
    "\n",
    "#Estas son las columnas seleccionadas en el proyecto base para cada pipeline.\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"CouncilArea\",'Type','Suburb','Postcode']\n",
    "\n",
    "#Creamos la \"full_pipeline\", es decir la pipeline que engloba a todas las otras.\n",
    "full_naif_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_naif_pipeline, num_attribs),\n",
    "        (\"cat\", cat_naif_pipeline, cat_attribs),\n",
    "])\n",
    "#Preprocesamos los datos utilizando la \"full_pipeline\"\n",
    "housing_prepared = full_naif_pipeline.fit_transform(X_train,y_train)\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "\n",
    "n_neighbors = 3\n",
    "knn_reg = neighbors.KNeighborsRegressor(n_neighbors)\n",
    "knn_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "models = [(lin_reg,\"lin_reg\"),\n",
    "          (knn_reg,\"KNN-Regressor\"),\n",
    "          (forest_reg,'Random Forest')\n",
    "]\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    scores = cross_val_score(model[0], housing_prepared, housing_labels, scoring=\"neg_root_mean_squared_error\", cv=5,n_jobs=-1)\n",
    "    display_scores(-scores, model[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarda mucho en ejecutarse (Si quieres ejecutarlo simplemente descomenta el for que está justo al final de la celda anterior.), así que aquí pongo los resultados:\n",
    "\n",
    "    -lin_reg:\n",
    "        Mean: 384025.51472573285\n",
    "        Standard deviation: 48252.21371717167\n",
    "    -KNN-Regressor:\n",
    "        Mean: 375914.6862240796\n",
    "        Standard deviation: 31220.851831198084\n",
    "    -Random Forest ----\n",
    "        Mean: 310410.0826121716\n",
    "        Standard deviation: 36235.270271245245\n",
    "        \n",
    "Estos serán nuestros resultados base, nuestro punto de referencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_knn = X_train.copy()\n",
    "y_train_knn = y_train.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regresión Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lr = X_train.copy()\n",
    "y_train_lr = y_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf = X_train.copy()\n",
    "y_train_rf = y_train.copy()\n",
    "model = RandomForestRegressor(random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared:  (4345, 516)\n"
     ]
    }
   ],
   "source": [
    "# Esta será la pipeline para el modelo Random Forest\n",
    "\n",
    "#Para las columnas con muchos ceros sin sentido. A las que además se les aplicará la función logaritmo\n",
    "num0_pipeline = Pipeline([\n",
    "        ('zeros2NaN',FunctionTransformer(func = replace_0_2_NaN,validate=False)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('log',FunctionTransformer(np.log1p, validate=True)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "#Para las otras columnas numéricas\n",
    "num1_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "#Para las variables categóricas. Principalmente utilizaremos el OneHotEncoder\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"constant\",fill_value='Unknown')),\n",
    "        ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "\"\"\"\n",
    "Columnas numéricas:  ['Rooms', 'Price', 'Distance', 'Bedroom2', 'Bathroom', 'Car', \n",
    "'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "\n",
    "Columnas categóricas:  ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
    "\"\"\"\n",
    "\n",
    "#Estas son las columnas seleccionadas por mi para cada pipeline.\n",
    "num_attribs0 = ['Landsize','BuildingArea']\n",
    "num_attribs1 = ['Distance', 'Bedroom2', 'Bathroom', 'Car', \n",
    "'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "cat_attribs = [\"CouncilArea\",'Type','Suburb','Postcode']\n",
    "\n",
    "#Creamos la \"full_pipeline\", es decir la pipeline que engloba a todas las otras.\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num0\", num0_pipeline, num_attribs0),\n",
    "        (\"num1\", num1_pipeline, num_attribs1),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "])\n",
    "\n",
    "#Preprocesamos los datos utilizando la \"full_pipeline\"\n",
    "X_train_prepared = full_pipeline.fit_transform(X_train_rf,y_train_rf)\n",
    "print(\"Prepared: \",X_train_prepared.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trained model:\n",
      "RandomForestRegressor(max_features=110, n_estimators=82, random_state=100)\n",
      "Best parameters:\n",
      "{'max_depth': None, 'max_features': 110, 'n_estimators': 82}\n",
      "Best Score\n",
      "550.1148792382141 302626.380359275\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(random_state=100)\n",
    "rf_param_grid = {\n",
    "                'max_features': range(10,120,10),\n",
    "                'n_estimators': [82],\n",
    "                    'max_depth': [\"sqrt\",None],\n",
    "                   #'bootstrap': [False],\n",
    "                #\"min_samples_split\":[2,3],\n",
    "                #\"min_samples_leaf\":[1,2],\n",
    "}\n",
    "rf_grid_search = GridSearchCV(model, param_grid=rf_param_grid, cv=5,\n",
    "                           scoring='neg_root_mean_squared_error',\n",
    "                           return_train_score=True,n_jobs=-1)\n",
    "rf_grid_search.fit(X_train_prepared, y_train)\n",
    "\n",
    "print(\"Best trained model:\")\n",
    "print(rf_grid_search.best_estimator_)\n",
    "print(\"Best parameters:\")\n",
    "print(rf_grid_search.best_params_)\n",
    "print(\"Best Score\")\n",
    "print(np.sqrt(-rf_grid_search.best_score_),-rf_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
