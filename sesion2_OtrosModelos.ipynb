{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje automático \n",
    "# Proyecto 2: Housing prediction ML models\n",
    "### Miguel Augusto March Piris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# some imports\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "plt.rc('font', size=12) \n",
    "plt.rc('figure', figsize = (12, 5))\n",
    "\n",
    "# Settings for the visualizations\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2,'font.family': [u'times']})\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 25)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "# create output folder\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "if not os.path.exists('output/session1'):\n",
    "    os.makedirs('output/session1')\n",
    "    \n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import neighbors\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "## lets comare all of them \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algunas funciones no relevantes para el tema tratado: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to display score results from CV\n",
    "def display_scores(scores,model_name = None):\n",
    "    if(model_name):\n",
    "        print(\"----\",model_name,\"----\")\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "#Function to plat the values of a gridsearch with only one parameter changing. \n",
    "def plot_results(results,param_grid,variable_name,xlim = None,ylim = None):\n",
    "    #plot the results\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.title(\"GridSearchCV\",\n",
    "              fontsize=16)\n",
    "\n",
    "    values = param_grid[variable_name]\n",
    "    \n",
    "    plt.xlabel(variable_name)\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    ax = plt.gca()\n",
    "    #ax.set_xlim(min_v, max_v)\n",
    "    if xlim:\n",
    "        min_v, max_v = xlim\n",
    "        ax.set_xlim(min_v, max_v)\n",
    "    if ylim:\n",
    "        min_v, max_v = ylim\n",
    "        ax.set_ylim(min_v, max_v)\n",
    "\n",
    "    # Get the regular numpy array from the MaskedArray\n",
    "    X_axis = np.array(results['param_'+variable_name].data, dtype=float)\n",
    "\n",
    "\n",
    "    for sample, style in (('train', '--'), ('test', '-')):\n",
    "        sample_score_mean = (-results['mean_%s_score' % (sample)])\n",
    "        sample_score_std = (results['std_%s_score' % (sample)])\n",
    "        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
    "                        sample_score_mean + sample_score_std,\n",
    "                        alpha=0.1 if sample == 'test' else 0)\n",
    "        ax.plot(X_axis, sample_score_mean, style,\n",
    "                alpha=1 if sample == 'test' else 0.7,\n",
    "                label=\"(%s)\" % ( sample))\n",
    "\n",
    "    best_index = np.nonzero(results['rank_test_score' ] == 1)[0][0]\n",
    "    best_score =  (-results['mean_test_score' ][best_index])\n",
    "\n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    ax.plot([X_axis[best_index], ] * 2, [best_score, best_score],\n",
    "            linestyle='-.',  marker='x', markeredgewidth=3, ms=8)\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    ax.annotate(\"%0.2f\" % best_score,\n",
    "                (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_att_dic(list_att):\n",
    "    dic_att={}\n",
    "    for key in initial_atr.keys():\n",
    "        dic_att[key]=[]\n",
    "    for (k,v) in list_att:\n",
    "        dic_att[k].append(v)\n",
    "    return dic_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output identical at every run\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading datasets\n",
    "housing = pd.read_csv('dataset/housing-snapshot/train_set.csv',index_col=0) \n",
    "test_housing = pd.read_csv('dataset/housing-snapshot/test_set.csv',index_col=0)\n",
    "#Postcode is in reality a categorical feature\n",
    "housing['Postcode'] = pd.Categorical(housing.Postcode)\n",
    "\n",
    "## divide using the scikit learn function\n",
    "housing[\"price_cat\"] = pd.cut(housing[\"Price\"],\n",
    "                               bins=[0., 500000, 1000000, 1500000, 2000000., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"price_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "def price_cat_proportions(data):\n",
    "    return data[\"price_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": price_cat_proportions(housing),\n",
    "    \"Stratified\": price_cat_proportions(strat_test_set),\n",
    "    \"Random\": price_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n",
    "\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"price_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para cuando quiero el dataset nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = strat_train_set.drop(\"Price\", axis=1).copy()\n",
    "y_train = strat_train_set[\"Price\"].copy()\n",
    "housing = strat_train_set.drop(\"Price\", axis=1).copy()\n",
    "housing_labels = strat_train_set[\"Price\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipe del proyecto base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing.select_dtypes(include=[np.number]).columns\n",
    "housing_cat = housing.select_dtypes(include=[np.object]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4345x517 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 82555 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create a function to replace 0 by NaN\n",
    "def replace_0_2_NaN(data):\n",
    "    data[data == 0] = np.nan\n",
    "    return data\n",
    "\n",
    "\n",
    "# column index\n",
    "Rooms_ix, Bedroom2_ix, Bathroom_ix, BuildingArea_ix = 0, 2, 3, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        rooms_per_building_area = X[:, Rooms_ix] / (1.0 +X[:, BuildingArea_ix])# add 1 to avoid 0 division\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, Bedroom2_ix] / (1.0 + X[:, Bathroom_ix]) # add 1 to avoid 0 division\n",
    "            return np.c_[X, rooms_per_building_area, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_building_area]\n",
    "#Para las columnas con muchos ceros sin sentido. A las que además se les aplicará la función logaritmo\n",
    "num0_pipeline = Pipeline([\n",
    "        ('zeros2NaN',FunctionTransformer(func = replace_0_2_NaN,validate=False)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('log',FunctionTransformer(np.log1p, validate=True)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "#Para las otras columnas numéricas\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "#Para las variables categóricas. Principalmente utilizaremos el OneHotEncoder\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"constant\",fill_value='Unknown')),\n",
    "        ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\"\"\"\n",
    "Columnas numéricas:  ['Rooms', 'Price', 'Distance', 'Bedroom2', 'Bathroom', 'Car', \n",
    "'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "\n",
    "Columnas categóricas:  ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
    "\"\"\"\n",
    "\n",
    "#Estas son las columnas seleccionadas en el proyecto base para cada pipeline.\n",
    "num_attribs0 = ['Landsize','BuildingArea']\n",
    "num_attribs1 = list(housing_num)\n",
    "cat_attribs = [\"CouncilArea\",'Type','Suburb','Postcode']\n",
    "\n",
    "#Creamos la \"full_pipeline\", es decir la pipeline que engloba a todas las otras.\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num0\", num0_pipeline, num_attribs0),\n",
    "        (\"num1\", num_pipeline, num_attribs1),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "])\n",
    "#Preprocesamos los datos utilizando la \"full_pipeline\"\n",
    "housing_prepared = full_pipeline.fit_transform(housing,housing_labels)\n",
    "housing_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferentes modelos de base\n",
    "\n",
    "En la celda siguiente se ejecutan los 5 modelos distintos que podíamos utilizar para este proyecto. Aunque los resultados no son definitivos porque entran muchas variables distintas a la hora de discernir que modelo es el el mejor, podemos hacernos una primera idea de cuales son las posibilidades de cada modelo.\n",
    "\n",
    "Para ello hacemos uso de los modelos sin tocar mucho los diferentes hiperparámetros y utilizamos para tener más consistencia en los resultados de esta prueba la función \n",
    "\n",
    "El código está comentado para evitar que el notebook se quede mucho tiempo atascado ejecutando esta celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nforest_reg = RandomForestRegressor(n_estimators=20, random_state=42)\\nforest_reg.fit(housing_prepared, housing_labels)\\n\\nlin_reg = LinearRegression()\\nlin_reg.fit(housing_prepared, housing_labels)\\n\\ntree_reg = DecisionTreeRegressor(random_state=42)\\ntree_reg.fit(housing_prepared, housing_labels)\\n\\nn_neighbors = 3\\nknn_reg = neighbors.KNeighborsRegressor(n_neighbors)\\nknn_reg.fit(housing_prepared, housing_labels)\\n\\n\\nsvm_reg = SVR(kernel=\"linear\")\\nsvm_reg.fit(housing_prepared, housing_labels)\\n\\nmodels = [(lin_reg,\"lin_reg\"),\\n          (tree_reg,\"Decision Tree\"),\\n          (knn_reg,\"KNN-Regressor\"),\\n          (forest_reg,\\'Random Forest\\'),\\n          (svm_reg,\\'SVM Regressor\\')\\n]\\nfor model in models:\\n    scores = cross_val_score(model[0], housing_prepared, housing_labels, scoring=\"neg_root_mean_squared_error\", cv=10,n_jobs=-1)\\n    print(-scores.mean())\\n    display_scores(-scores, model[1])\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "forest_reg = RandomForestRegressor(n_estimators=20, random_state=42)\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "n_neighbors = 3\n",
    "knn_reg = neighbors.KNeighborsRegressor(n_neighbors)\n",
    "knn_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "\n",
    "svm_reg = SVR(kernel=\"linear\")\n",
    "svm_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "models = [(lin_reg,\"lin_reg\"),\n",
    "          (tree_reg,\"Decision Tree\"),\n",
    "          (knn_reg,\"KNN-Regressor\"),\n",
    "          (forest_reg,'Random Forest'),\n",
    "          (svm_reg,'SVM Regressor')\n",
    "]\n",
    "for model in models:\n",
    "    scores = cross_val_score(model[0], housing_prepared, housing_labels, scoring=\"neg_root_mean_squared_error\", cv=10,n_jobs=-1)\n",
    "    print(-scores.mean())\n",
    "    display_scores(-scores, model[1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados:\n",
    "\n",
    "- Linear Regressor:\n",
    "\n",
    "    - Mean: 380655.96847603103\n",
    "    - Standard deviation: 49217.88546072668\n",
    "\n",
    "- Decision Tree:\n",
    "\n",
    "    - Mean: 421726.4131430878\n",
    "    - Standard deviation: 49564.46361021512\n",
    "\n",
    "- K-Nearest Neigbours (KNN) Regressor:\n",
    "\n",
    "    - Mean: 369372.76227516925\n",
    "    - Standard deviation: 49121.12777363014\n",
    "\n",
    "- Random Forest:\n",
    "\n",
    "    - Mean: 308652.50854966586\n",
    "    - Standard deviation: 44595.78784796293\n",
    "\n",
    "- Support Vector Machine (SVM) Regressor:\n",
    "\n",
    "    - Mean: 660668.7269286377\n",
    "    - Standard deviation: 54076.75231812296\n",
    "\n",
    "#### Reflexión:\n",
    "\n",
    "Comparando los resultados podemos intuir que ni el modelo SVM ni el Decision Tree tienen muy buenas oportunidades con el problema que estamos tratando. \n",
    "\n",
    "Por otro lado el Random Forest, que podríamos decir que es una versión más complicada del modelo Decision Tree, si que tiene buenos pronósticos. Además le siguen de cerca el KNN y el Lineal Regressor.\n",
    "\n",
    "También podría ser que la pipeline utilizada para el preproceso de datos estuviera beneficiando mucho a los modelos que salen mejor parados o viceversa. Es por ello que más adelante probaremos un sistema de Feature Selection para ver como se comportan los modelos cuando las pipelines se adaptan a ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mi pipeline:\n",
    "\n",
    "### Invert variable:\n",
    "\n",
    "Es una función que por cada columa de entrada, habrá dos columnas de salida. La primera será igual y la segunda será 1/columna.\n",
    "Utilizando esta función junto a la función _PolynomialFeatures(interaction_only=True,degree=2))_ conseguimos unas interacciones que podrían resultar interesantes. Ya no solo col1 * col2, sino también col1/col2, col2/col1 y 1/(col1 * col2).\n",
    "\n",
    "### Categorical to Mean:\n",
    "\n",
    "Es una función que nos convertirá las columnas categóricas en valores numéricos. Para ser concretos calculará la media del precio para cada uno de los valores.\n",
    "\n",
    "Por ejemplo: Para la columna type, se calcularía la media del precio de todas las filas que tienen el valor **_t_** y se sustituiría por todas las **_t_**'s. Así con todos los valores únicos de la columna Type.\n",
    "\n",
    "### Clean Outlayers:\n",
    "\n",
    "Es una función que limpia los Outlayers basándose en los quantiles.\n",
    "\n",
    "### Problemas:\n",
    "\n",
    "Un problema que he detectado con mi pipeline es que si vamos a utilizar una función para hacer un CV tenemos un problema importante. Esto es así porque en _Categorical_2_mean()_ calculamos la media del precio asociado a cada valor de las columnas categóricas, de manera que cada fila contiene (potencialmente) información relevante de otras muchas filas. Dado que el preproceso de datos se hace antes de enviar los datos a la función CV tenemos un problema muy grande porque dentro de CV los datos ya están mezclados y da igual que haga la division entre train y test, porque el train ya va a tener información que se puede extraer del test.\n",
    "\n",
    "Para hacer esto correctamente tendría que haber hecho mi propia función CV o encontrar alguna que me permitiera pasarle la pipeline. Dado que me di cuenta después de entregar el proyecto y que al final no utilicé la pipeline con esa función no lo he programado.\n",
    "\n",
    "Me di cuenta de este problema al estar haciendo unas pruebas y encontrarme que la media de los scores estaba por debajo de 50.000, cosa que no tenía ningún sentido comparándolo con los resultados anteriores.\n",
    "\n",
    "\n",
    "Otro problema que he encontrado es que la función CleanOutlayers no funciona demasiado bien con el Random Forest Regressor. He leído en internet que la forma que tiene de aislar los Outlayers el Random Forest en sus ramas es suficientemente robusta como para no tener que limpiarlos de esta manera. Quizá por ello los resultados empeoraban al utilizar esta función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared:  (4345, 537)\n"
     ]
    }
   ],
   "source": [
    "class invert_variable(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # no *args or **kargs\n",
    "        pass\n",
    "        #self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        data=np.zeros(X.shape,dtype=np.float)\n",
    "        for i in range(X.shape[1]):\n",
    "            data[:, 0]=1/X[:, 0]\n",
    "        return np.c_[X,data]\n",
    "class Categorical_2_mean(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # no *args or **kargs\n",
    "        self.medias_por_columnas=[]\n",
    "        self.media_y=0\n",
    "        #self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.media_y = y.mean()\n",
    "        self.medias_por_columnas=[]\n",
    "        #Para cada columna\n",
    "        for i in range(X.shape[1]):\n",
    "            medias = {}\n",
    "            columna = X[:,i]\n",
    "            unicos = np.unique(columna)\n",
    "            #Para cada valor guardo la media\n",
    "            for u in unicos:\n",
    "                medias[u]=y[columna==u].mean()\n",
    "                \n",
    "            self.medias_por_columnas.append(medias)\n",
    "            \n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        data = np.ones((X.shape[0]))\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            columna = X[:,i]\n",
    "            media = self.medias_por_columnas[i]\n",
    "            nueva_columna = np.zeros(X.shape[0])+self.media_y\n",
    "            \n",
    "            #Para cada valor distinto dentro de la categoría\n",
    "            for k in media.keys():\n",
    "                nueva_columna[columna == k] = media[k]\n",
    "            \n",
    "            #Si es la primera vez\n",
    "            if i == 0:\n",
    "                data = nueva_columna\n",
    "            else:\n",
    "                data = np.c_[data,nueva_columna]\n",
    "        return data\n",
    "\n",
    "class Clean_Outlayers_Quantile(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,q=0.01): # no *args or **kargs\n",
    "        self.q=q\n",
    "        self.low_q_col=[]\n",
    "        self.high_q_col=[]\n",
    "        #self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        #Para cada columna\n",
    "        for i in range(X.shape[1]):\n",
    "            columna = X[:,i]\n",
    "            self.high_q_col.append(np.quantile(a=columna , q=1-self.q))\n",
    "            self.low_q_col.append(np.quantile(a=columna ,q=self.q))\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        for i in range(X.shape[1]):\n",
    "            q_high=self.high_q_col[i]\n",
    "            q_low =self.low_q_col[i]\n",
    "            columna = X[:,i]\n",
    "            columna[columna>q_high]=q_high\n",
    "            columna[columna<q_low]=q_low\n",
    "        return X\n",
    "\n",
    "# Primero el preprocesamiento\n",
    "rf_pipe_num0 = Pipeline([\n",
    "    ('zeros2NaN',FunctionTransformer(func = replace_0_2_NaN,validate=False)),\n",
    "    (\"fill_nan\",SimpleImputer(strategy=\"mean\")),\n",
    "    (\"clean_outlayer\",Clean_Outlayers_Quantile()),\n",
    "    ('log',FunctionTransformer(np.log1p, validate=True)),\n",
    "    (\"std\",StandardScaler()),\n",
    "])\n",
    "\n",
    "rf_pipe_num1 = Pipeline([\n",
    "    (\"fill_nan\",SimpleImputer(strategy=\"mean\")),\n",
    "    (\"clean_outlayer\",Clean_Outlayers_Quantile()),\n",
    "    (\"std\",StandardScaler()),\n",
    "])\n",
    "\n",
    "rf_pipe_poli = Pipeline([\n",
    "    ('zeros2NaN',FunctionTransformer(func = replace_0_2_NaN,validate=False)),\n",
    "    (\"fill_nan\",SimpleImputer(strategy=\"mean\")),\n",
    "    ('zeros2NaN_2',FunctionTransformer(func = replace_0_2_NaN,validate=False)),# por si acaso hay algun 0\n",
    "    (\"invert_1/var\",invert_variable()),\n",
    "    (\"poly_interact_2\",PolynomialFeatures(interaction_only=True,degree=2)),\n",
    "    (\"std\",StandardScaler()),\n",
    "])\n",
    "\n",
    "rf_cat_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"constant\",fill_value='Unknown')),\n",
    "        ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "# \n",
    "rf_cat2mean_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"constant\",fill_value='Unknown')),\n",
    "        ('cat2mean', Categorical_2_mean()),\n",
    "        ('zeros2NaN',FunctionTransformer(func = replace_0_2_NaN,validate=False)),\n",
    "        (\"std\",StandardScaler()),\n",
    "])\n",
    "\"\"\"\n",
    "NUMERICAL:    ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea',\n",
    "'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "CATEGORICAL:  ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
    "\"\"\"\n",
    "num_attribs0 = ['Landsize','BuildingArea']\n",
    "num_attribs1 = ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car',\n",
    "'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "\n",
    "num_attribs_poli = ['Rooms','BuildingArea']\n",
    "\n",
    "cat_attribs =['Type',\"CouncilArea\",'Suburb','Regionname','Postcode']\n",
    "\n",
    "rf_full_pipe = ColumnTransformer([\n",
    "    (\"num0\", rf_pipe_num0, num_attribs0),\n",
    "    (\"num1\", rf_pipe_num1, num_attribs1),\n",
    "    (\"poli\", rf_pipe_poli, num_attribs_poli),\n",
    "    (\"cat\",  rf_cat_pipe, cat_attribs),\n",
    "    (\"cat_2_mean\",  rf_cat2mean_pipe, cat_attribs)\n",
    "])\n",
    "\n",
    "X_train_prepared = rf_full_pipe.fit_transform(X_train,y_train)\n",
    "\n",
    "X_test_prepared = rf_full_pipe.transform(test_housing)\n",
    "print(\"Prepared: \",X_train_prepared.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Feature Selection:\n",
    "\n",
    "### Explicación:\n",
    "Vamos a utilizar un sistema de selección de características para ver como pueden mejorar los modelos si adaptamos las features con las que trabajan. \n",
    "\n",
    "En esta explicación yo llamo atributos a las columnas de entrada a la pipeline.\n",
    "\n",
    "En cada iteración el número total de atributos se mantiene.\n",
    "Para ser concretos vamos a ir quitando en una misma iteración de uno en uno todos los atributos y calculando con que atributos de entrada el modelo se comporta mejor. \n",
    "\n",
    "El resultado de la combinación que se comporte mejor será comparado con el resutado mejor de la iteración anterior, y si es mejor seguiremos con la siguiente iteración, pero quitando el atributo correpondiente de la lista de atributos. En caso contrario habremos acabado y tendremos el resultado final. \n",
    "En caso de que una de las sublistas de atributos se quede vacía esto significa que la pipeline a la que está asociada no funciona nada bien con el modelo que estamos probando.\n",
    "\n",
    "### Uso:\n",
    "\n",
    "Todo este proceso podremos repetirlo con distintas pipelines y con distintos modelos para intentar encontrar los atributos y modelos que consiguen mejores resultados sin tener que calcular todas las posibles combinaciones, que sería de una complejidad enorme.\n",
    "\n",
    "En mi caso lo he utilizado para asegurarme de que ningún modelo estaba funcionando especialmente mal por culpa de los atributos y de la pipeline y además, para refinar un poco más los modelos que mejores resultados daban en el GridSearch CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressor(max_depth=4)\n",
      "El primer tiempo (T1) solo se calcula con 1 CV, mientras que el resto tardarán aproximadamente T1 x N_atributos\n",
      "0 . Score:  413762.84402127645\n",
      "\n",
      "Tiempo(seg) cálculo: 2.489342212677002\n",
      "\n",
      "24  atributos: \n",
      "  - num_log_att ['Landsize', 'BuildingArea']\n",
      "  - num_att ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
      "  - num_poli_att ['Rooms', 'BuildingArea', 'Bathroom']\n",
      "  - cat_att ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
      "-------------------------------------------------------------------------------------------\n",
      "1 . Score:  413724.88125563506\n",
      "\n",
      "Tiempo(seg) cálculo: 4.764912843704224\n",
      "\n",
      "23  atributos: \n",
      "  - num_log_att ['Landsize', 'BuildingArea']\n",
      "  - num_att ['Rooms', 'Distance', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
      "  - num_poli_att ['Rooms', 'BuildingArea', 'Bathroom']\n",
      "  - cat_att ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
      "-------------------------------------------------------------------------------------------\n",
      "2 . Score:  413626.7840371605\n",
      "\n",
      "Tiempo(seg) cálculo: 3.1854794025421143\n",
      "\n",
      "22  atributos: \n",
      "  - num_log_att ['Landsize', 'BuildingArea']\n",
      "  - num_att ['Rooms', 'Distance', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
      "  - num_poli_att ['Rooms', 'BuildingArea']\n",
      "  - cat_att ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
      "-------------------------------------------------------------------------------------------\n",
      "3 . Score:  411069.81564338965\n",
      "\n",
      "Tiempo(seg) cálculo: 2.802530527114868\n",
      "\n",
      "21  atributos: \n",
      "  - num_log_att ['Landsize', 'BuildingArea']\n",
      "  - num_att ['Rooms', 'Distance', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
      "  - num_poli_att ['BuildingArea']\n",
      "  - cat_att ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
      "-------------------------------------------------------------------------------------------\n",
      "4 . Score:  411061.1576830917\n",
      "\n",
      "Tiempo(seg) cálculo: 2.500283718109131\n",
      "\n",
      "20  atributos: \n",
      "  - num_log_att ['Landsize', 'BuildingArea']\n",
      "  - num_att ['Rooms', 'Distance', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'Lattitude', 'Longtitude', 'Propertycount']\n",
      "  - num_poli_att ['BuildingArea']\n",
      "  - cat_att ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
      "-------------------------------------------------------------------------------------------\n",
      "5 . Score:  410503.07525433786\n",
      "\n",
      "Tiempo(seg) cálculo: 2.407559394836426\n",
      "\n",
      "19  atributos: \n",
      "  - num_log_att ['Landsize', 'BuildingArea']\n",
      "  - num_att ['Rooms', 'Distance', 'Bathroom', 'Car', 'BuildingArea', 'Lattitude', 'Longtitude', 'Propertycount']\n",
      "  - num_poli_att ['BuildingArea']\n",
      "  - cat_att ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
      "-------------------------------------------------------------------------------------------\n",
      "MEJOR SOLUCIÓN: \n",
      "{'num_log_att': ['Landsize', 'BuildingArea'], 'num_att': ['Rooms', 'Distance', 'Bathroom', 'Car', 'BuildingArea', 'Lattitude', 'Longtitude', 'Propertycount'], 'num_poli_att': ['BuildingArea'], 'cat_att': ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']}\n",
      "Score:  410503.07525433786\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Aquí tenemos todos los posibles atributos:\n",
    "NUMERICAL:    ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea',\n",
    "'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "CATEGORICAL:  ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
    "\"\"\"\n",
    "# Aquí van los distintos atributos que entrarán a las pipelines en un principio.\n",
    "# No tienen porqué estar todos los numéricos ni todos los categóricos en cada lista.\n",
    "\n",
    "num_log_att = ['Landsize','BuildingArea']\n",
    "\n",
    "num_att = ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea',\n",
    "'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "\n",
    "num_poli_att = ['Rooms','BuildingArea','Bathroom']\n",
    "\n",
    "cat_att =['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
    "\n",
    "initial_atr = {\n",
    "    \"num_log_att\":num_log_att.copy() ,\n",
    "    \"num_att\":num_att.copy() ,\n",
    "    \"num_poli_att\":num_poli_att.copy() ,\n",
    "    \"cat_att\":cat_att.copy() ,\n",
    "}\n",
    "\n",
    "initial_full_pipe = ColumnTransformer([\n",
    "            (\"num0\", rf_pipe_num0, initial_atr[\"num_log_att\"]),\n",
    "            (\"num1\", rf_pipe_num1, initial_atr[\"num_att\"]),\n",
    "            (\"poli\", rf_pipe_poli, initial_atr[\"num_poli_att\"]),\n",
    "            (\"cat\",  rf_cat_pipe, initial_atr[\"cat_att\"])\n",
    "])\n",
    "\n",
    "# Modelo: Este será el modelo con el que se calcule el la feature selection.\n",
    "# Deberíamos probarlos todos para ver si consiguen una mejora notable con un preprocesamiento de datos más adaptado.\n",
    "# Aquí muchos de los modelos están con parámetros bastante \"baratos\" para que se ejecute todo más rápido,\n",
    "# aún así se pueden tocar todos los parámetros de los diferentes modelos, pero esto implica tiempos muy largos de cálculo.\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth=4)\n",
    "#model = RandomForestRegressor(max_depth=10,n_estimators = 20  , random_state=42)\n",
    "#model = LinearRegression()\n",
    "#model = DecisionTreeRegressor(random_state=42)\n",
    "#model = SVR(kernel=\"linear\")\n",
    "\n",
    "#En caso de utilizar el KNN no podemos tener en cuenta la pipeline de categorías con OneHotEncoder, \n",
    "#pues sube demasiado la dimensionalidad. Por ello ponemos la lista vacía.\n",
    "#model = neighbors.KNeighborsRegressor(3)\n",
    "#cat_att =[]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Calculamos primero con todos los atributos.\n",
    "X_train_initial = initial_full_pipe.fit_transform(X_train,y_train)\n",
    "\n",
    "previous_score_winner = -cross_val_score(model, X_train_initial, y_train,scoring=\"neg_root_mean_squared_error\", cv=5,n_jobs=-1).mean()\n",
    "# Creamos una lista de tuplas (nombre_pipeline, nombre_columna) para un manejo más sencillo en el bucle.\n",
    "# Cada tupla representa un atributo de entrada a la pipeline.\n",
    "actual_list_atts = []    \n",
    "for k in initial_atr.keys():\n",
    "    for v in initial_atr[k]:\n",
    "        actual_list_atts.append((k,v))\n",
    "\n",
    "buscando = True\n",
    "cont=0\n",
    "print(model)\n",
    "while buscando and len(actual_list_atts)!=0:\n",
    "    \n",
    "    end = time.time()\n",
    "    tiempo=end - start\n",
    "    start = end\n",
    "    if cont == 0:\n",
    "        print(\"El primer tiempo (T1) solo se calcula con 1 CV, mientras que el resto tardarán aproximadamente T1 x N_atributos\")\n",
    "    dic_=create_att_dic(actual_list_atts)\n",
    "    print(cont,\". Score: \",previous_score_winner)\n",
    "    print()\n",
    "    print(\"Tiempo(seg) cálculo:\" ,tiempo )\n",
    "    print()\n",
    "    print(len(actual_list_atts),\" atributos: \")\n",
    "    for k in dic_.keys():\n",
    "        print(\"  -\",k,dic_[k])\n",
    "    print(\"-------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    scores_ronda=[]\n",
    "    for at in actual_list_atts:\n",
    "        actual_list_without_at = actual_list_atts.copy()\n",
    "        actual_list_without_at.remove(at)\n",
    "        #Aqui hemos quitado la feature\n",
    "        dict_att=create_att_dic(actual_list_without_at)\n",
    "        #Creamos la nueva pipe sin dicho atributo\n",
    "        actual_full_pipe = ColumnTransformer([\n",
    "            (\"num0\", rf_pipe_num0, dict_att[\"num_log_att\"]),\n",
    "            (\"num1\", rf_pipe_num1, dict_att[\"num_att\"]),\n",
    "            (\"poli\", rf_pipe_poli, dict_att[\"num_poli_att\"]),\n",
    "            (\"cat\",  rf_cat_pipe, dict_att[\"cat_att\"])\n",
    "        ])\n",
    "        #Procesamos los datos con la nueva pipe\n",
    "        X_train_actual = actual_full_pipe.fit_transform(X_train,y_train)\n",
    "        #Calculamos con el modelo y guardamos como de bueno es\n",
    "        actual_score = - cross_val_score(model, X_train_actual, y_train,\n",
    "                                         scoring=\"neg_root_mean_squared_error\", cv=5,n_jobs=-1).mean()\n",
    "        scores_ronda.append((actual_score,at))\n",
    "    \n",
    "    winner = min(scores_ronda, key= lambda x : x[0])\n",
    "    #Aquí tenemos el mejor score y el atributo que hay que quitar para tenerlo\n",
    "    score_winner, att_winner = winner\n",
    "    \n",
    "    #Si este resultado es mejor que el mejor de la ronda anterior\n",
    "    if score_winner < previous_score_winner:\n",
    "        actual_list_atts.remove(att_winner)\n",
    "        previous_score_winner = score_winner\n",
    "        \n",
    "    #Si no es mejor hemos acabado\n",
    "    else:\n",
    "        buscando=False\n",
    "    cont+=1\n",
    "    \n",
    "final_att = create_att_dic(actual_list_atts)\n",
    "print(\"MEJOR SOLUCIÓN: \")\n",
    "print(final_att)\n",
    "print(\"Score: \",previous_score_winner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo de la salida de la celda anterior en caso de no tener tiempo de ejecutarla:\n",
    "\n",
    "RandomForestRegressor(max_depth=10, n_estimators=20, random_state=42)\n",
    "El primer tiempo (T1) solo se calcula con 1 CV, mientras que el resto tardarán aproximadamente T1 x N_atributos\n",
    "0 . Score:  320939.4529929297\n",
    "\n",
    "Tiempo(seg) cálculo: 6.4403440952301025\n",
    "\n",
    "24  atributos: \n",
    "  - num_log_att ['Landsize', 'BuildingArea']\n",
    "  - num_att ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "  - num_poli_att ['Rooms', 'BuildingArea', 'Bathroom']\n",
    "  - cat_att ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
    "-------------------------------------------------------------------------------------------\n",
    "1 . Score:  320803.46586287156\n",
    "\n",
    "Tiempo(seg) cálculo: 87.13541197776794\n",
    "\n",
    "23  atributos: \n",
    "  - num_log_att ['Landsize', 'BuildingArea']\n",
    "  - num_att ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "  - num_poli_att ['Rooms', 'BuildingArea', 'Bathroom']\n",
    "  - cat_att ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
    "-------------------------------------------------------------------------------------------\n",
    "2 . Score:  320055.736743595\n",
    "\n",
    "Tiempo(seg) cálculo: 80.07244563102722\n",
    "\n",
    "22  atributos: \n",
    "  - num_log_att ['Landsize', 'BuildingArea']\n",
    "  - num_att ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "  - num_poli_att ['Rooms', 'BuildingArea', 'Bathroom']\n",
    "  - cat_att ['Suburb', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
    "-------------------------------------------------------------------------------------------\n",
    "3 . Score:  317347.9045733091\n",
    "\n",
    "Tiempo(seg) cálculo: 54.85602951049805\n",
    "\n",
    "21  atributos: \n",
    "  - num_log_att ['Landsize', 'BuildingArea']\n",
    "  - num_att ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "  - num_poli_att ['Rooms', 'BuildingArea', 'Bathroom']\n",
    "  - cat_att ['Suburb', 'Type', 'Method', 'SellerG', 'CouncilArea', 'Regionname']\n",
    "-------------------------------------------------------------------------------------------\n",
    "4 . Score:  315566.2465887208\n",
    "\n",
    "Tiempo(seg) cálculo: 50.84276008605957\n",
    "\n",
    "20  atributos: \n",
    "  - num_log_att ['Landsize', 'BuildingArea']\n",
    "  - num_att ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "  - num_poli_att ['Rooms', 'BuildingArea', 'Bathroom']\n",
    "  - cat_att ['Suburb', 'Type', 'Method', 'SellerG', 'CouncilArea', 'Regionname']\n",
    "-------------------------------------------------------------------------------------------\n",
    "5 . Score:  315009.63095124095\n",
    "\n",
    "Tiempo(seg) cálculo: 45.56109118461609\n",
    "\n",
    "19  atributos: \n",
    "  - num_log_att ['Landsize', 'BuildingArea']\n",
    "  - num_att ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Landsize', 'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "  - num_poli_att ['Rooms', 'BuildingArea', 'Bathroom']\n",
    "  - cat_att ['Suburb', 'Type', 'Method', 'SellerG', 'CouncilArea', 'Regionname']\n",
    "-------------------------------------------------------------------------------------------\n",
    "MEJOR SOLUCIÓN: \n",
    "\n",
    "{\n",
    "\n",
    "        'num_log_att': ['Landsize', 'BuildingArea'], \n",
    "\n",
    "        'num_att': ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Landsize', 'YearBuilt', 'Lattitude', 'Longtitude'], \n",
    "\n",
    "        'num_poli_att': ['Rooms', 'BuildingArea', 'Bathroom'], \n",
    "\n",
    "        'cat_att': ['Suburb', 'Type', 'Method', 'SellerG', 'CouncilArea', 'Regionname']\n",
    "\n",
    "}\n",
    "\n",
    "Score:  315009.63095124095"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones feature selection:\n",
    "\n",
    "Utilizando este método con los distintos modelos podemos ver claramente que el Random Forest tiene las de ganar en este problema. Además, una vez encontremos unos buenos parámetros con ese modelo podremos volver a ejecutar esta celda con dichos parámetros y ver si conseguimos así un modelo aún mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation Grid-Search:\n",
    "\n",
    "### Explicación:\n",
    "\n",
    "El Grid-Search es un método que nos permite hacer una búsqueda de los mejores parámetros para un modelo. Ejecutará una CV con cada combinación de parámetros de manera que tenemos una seguridad un mayor del resultado de esos parámetros.\n",
    "\n",
    "### Pipeline definitiva:\n",
    "\n",
    "La pipeline escogida es la pipeline base del proyecto con la diferencia de que he modificado los atributos que entran a dicha pipeline. Esto es así porque al probar distintas pipelines esta ha sido la que ha resultado el mejor score con diferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta lista servirá para llevar la cuenta de los últimos mejores resultados.\n",
    "lista_resultados=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estas son las columnas seleccionadas por mi para cada pipeline.\n",
    "#Dado que la pipeline del proyecto base daba mejores resultados me he quedado con ella.\n",
    "num_attribs0 = ['Landsize','BuildingArea']\n",
    "num_attribs1 = ['Distance', 'Bedroom2', 'Bathroom', 'Car', \n",
    "'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "cat_attribs = [\"CouncilArea\",'Type','Suburb','Postcode']\n",
    "\n",
    "#Creamos la \"full_pipeline\", es decir la pipeline que engloba a todas las otras.\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num0\", num0_pipeline, num_attribs0),\n",
    "        (\"num1\", num_pipeline, num_attribs1),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4345, 516)\n"
     ]
    }
   ],
   "source": [
    "X_train_prepared = full_pipeline.fit_transform(X_train,y_train)\n",
    "# Como los mejores resultados en la feature selection los ha tenido el Random Forest vamos a hacer le grid-search con él.\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "print(X_train_prepared.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí he probado valores y rangos muy distintos. He intentado evitar hacer GridSearch's con varios parámetros con muchos valores, pues las combinaciones totales aumentan con un productorio de las combinaciones en un solo parámetro.\n",
    "\n",
    "Finalmente, aunque ha sido útil para quedarme con las mejores soluciones, no ha sido solo el valor que retornaba el CV GridSearch lo que he utilizado para decidir que solución era la mejor entre todas las otras, sino que también me he dejado guiar por el resultado del Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aquí podemos poner distintos valores de los parámetros de nuesto modelo.\n",
    "#Como estamos utilizando el Random Forest Regressor los valores que hay aquí corresponden a sus parámetros.\n",
    "rf_param_grid = {\n",
    "                'max_features': range(8,168,20),\n",
    "                'n_estimators': [82],\n",
    "                    'max_depth': [None],\n",
    "                   'bootstrap': [False],\n",
    "                #\"min_samples_split\":[2,3],\n",
    "                #\"min_samples_leaf\":[1,2],\n",
    "}\n",
    "rf_grid_search = GridSearchCV(model, param_grid=rf_param_grid, cv=5,\n",
    "                           scoring='neg_root_mean_squared_error',\n",
    "                           return_train_score=True,n_jobs=-1)\n",
    "rf_grid_search.fit(X_train_prepared, y_train)\n",
    "\n",
    "print(\"Best trained model:\")\n",
    "print(rf_grid_search.best_estimator_)\n",
    "print(\"Best parameters:\")\n",
    "print(rf_grid_search.best_params_)\n",
    "print(\"Best Score\")\n",
    "print(np.sqrt(-rf_grid_search.best_score_),-rf_grid_search.best_score_)\n",
    "lista_resultados.append((\"SCORE: \",-rf_grid_search.best_score_,rf_grid_search.best_params_,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_resultados.sort(key=lambda x : x[1] )\n",
    "lista_resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con esta función podemos hacer una visualización de los resultados respecto a un solo parámetro cambiante.\n",
    "plot_results(rf_grid_search.cv_results_,rf_param_grid,\"max_features\",ylim=(280000,320000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular la solución y guardarla en un fichero _.csv_ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_housing\n",
    "X_test_prepared = full_pipeline.transform(test_housing)\n",
    "\n",
    "# El modelo solución: \n",
    "model=RandomForestRegressor(max_features=128,n_estimators=82,max_depth=None,bootstrap=False,random_state=42)\n",
    "model.fit(X_train_prepared, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_prepared)\n",
    "\n",
    "df_output = pd.DataFrame(y_pred)\n",
    "df_output = df_output.reset_index()\n",
    "df_output.columns = ['index','Price']\n",
    "\n",
    "df_output.to_csv('output/prediciones.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones finales:\n",
    "\n",
    "Creo que todos los modelos que hemos visto son potentes y tienen su utilidad, sino no seguirían existiendo.\n",
    "En este caso el Random Forest es el que mejores resultados ha conseguido al principio. Aunque este parecía prometedor hemos tenido que hacer algunas comprobaciones para no caer en la trampa de escoger el primero que daba un buen resultado. \n",
    "\n",
    "Después de comprobar si podría ser culpa del preprocesamiento de los datos nos ha quedado un poco más claro que el Random Forest seguía manteniendo una buena ventaja frente a sus competidores:\n",
    "\n",
    "*     El SVM ha sido descartado por sus pésimos resultados.\n",
    "\n",
    "*     El Decision Tree ha sido descartado porque el Random Forest es una versión más refinada y robusta del Decisión Tree y además estaba dando mejores resultados.\n",
    "\n",
    "*     El Linear Regressor no tenía malos resultados pero no se podía trabajar demasiado más con él, pues ya lo había intentado con el Feature Selection y no había conseguido superar al Random Forest.\n",
    "\n",
    "*     El KNN Regressor, aunque tenía una puntuación similar al Linear Regressor aún se podría modificar cambiando el parámetro n_neighbours. Haciendo unas pruebas con el GridSearchCV encontré que el resultado mejoraba, pero no lo suficiente como para alcanzar al Random Forest.\n",
    " \n",
    "*     Dado que al final solo nos interesa el mejor modelo, la única opción viable era refinar el Random Forest al máximo y jugar con el GridSearchCV y con la Feature Selection hasta conseguir el mejor resultado posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
