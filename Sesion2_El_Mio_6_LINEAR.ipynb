{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# some imports\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "plt.rc('font', size=12) \n",
    "plt.rc('figure', figsize = (12, 5))\n",
    "\n",
    "# Settings for the visualizations\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2,'font.family': [u'times']})\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 25)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "# create output folder\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "if not os.path.exists('output/session1'):\n",
    "    os.makedirs('output/session1')\n",
    "    \n",
    "    \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import neighbors\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import time\n",
    "## lets comare all of them \n",
    "\n",
    "def display_scores(scores,model_name = None):\n",
    "    if(model_name):\n",
    "        print(\"----\",model_name,\"----\")\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "def plot_results(results,param_grid,variable_name):\n",
    "        #plot the results\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.title(\"GridSearchCV\",\n",
    "              fontsize=16)\n",
    "\n",
    "    values = param_grid[variable_name]\n",
    "    \n",
    "    min_v = min(values)\n",
    "    max_v = max(values)\n",
    "    \n",
    "    \n",
    "    plt.xlabel(variable_name)\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    ax = plt.gca()\n",
    "    #ax.set_xlim(min_v, max_v)\n",
    "\n",
    "\n",
    "    # Get the regular numpy array from the MaskedArray\n",
    "    X_axis = np.array(results['param_'+variable_name].data, dtype=float)\n",
    "\n",
    "\n",
    "    for sample, style in (('train', '--'), ('test', '-')):\n",
    "        sample_score_mean = (-results['mean_%s_score' % (sample)])\n",
    "        sample_score_std = (results['std_%s_score' % (sample)])\n",
    "        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
    "                        sample_score_mean + sample_score_std,\n",
    "                        alpha=0.1 if sample == 'test' else 0)\n",
    "        ax.plot(X_axis, sample_score_mean, style,\n",
    "                alpha=1 if sample == 'test' else 0.7,\n",
    "                label=\"(%s)\" % ( sample))\n",
    "\n",
    "    best_index = np.nonzero(results['rank_test_score' ] == 1)[0][0]\n",
    "    best_score =  (-results['mean_test_score' ][best_index])\n",
    "\n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    ax.plot([X_axis[best_index], ] * 2, [best_score, best_score],\n",
    "            linestyle='-.',  marker='x', markeredgewidth=3, ms=8)\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    ax.annotate(\"%0.2f\" % best_score,\n",
    "                (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output identical at every run\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv('dataset/housing-snapshot/train_set.csv',index_col=0) \n",
    "test_housing = pd.read_csv('dataset/housing-snapshot/test_set.csv',index_col=0) \n",
    "housing['Postcode'] = pd.Categorical(housing.Postcode)\n",
    "\n",
    "# For illustration only. Sklearn has train_test_split()\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "train_set, test_set = split_train_test(housing, 0.2)\n",
    "len(train_set),len(test_set)\n",
    "\n",
    "## Create a function that divides the data with an id\n",
    "## checks that id is not train and test set\n",
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
    "\n",
    "housing_with_id = housing.reset_index()   # adds an `index` column\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n",
    "\n",
    "## create an id base on latitude and longitude\n",
    "housing_with_id[\"id\"] = housing[\"Longtitude\"] * 1000 + housing[\"Lattitude\"]\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")\n",
    "\n",
    "## divide using the scikit learn function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "housing[\"price_cat\"] = pd.cut(housing[\"Price\"],\n",
    "                               bins=[0., 500000, 1000000, 1500000, 2000000., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"price_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "def price_cat_proportions(data):\n",
    "    return data[\"price_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": price_cat_proportions(housing),\n",
    "    \"Stratified\": price_cat_proportions(strat_test_set),\n",
    "    \"Random\": price_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n",
    "\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"price_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para cuando quiero el dataset nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = strat_train_set.drop(\"Price\", axis=1).copy()\n",
    "y_train = strat_train_set[\"Price\"].copy()\n",
    "\n",
    "housing = strat_train_set.drop(\"Price\", axis=1).copy()\n",
    "housing_labels = strat_train_set[\"Price\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipe del proyecto base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4345x517 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 82555 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "housing_num = housing.select_dtypes(include=[np.number])\n",
    "\n",
    "## create a function to replace 0 by NaN\n",
    "def replace_0_2_NaN(data):\n",
    "    data[data == 0] = np.nan\n",
    "    return data\n",
    "\n",
    "\n",
    "# column index\n",
    "Rooms_ix, Bedroom2_ix, Bathroom_ix, BuildingArea_ix = 0, 2, 3, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        rooms_per_building_area = X[:, Rooms_ix] / (1.0 +X[:, BuildingArea_ix])# add 1 to avoid 0 division\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, Bedroom2_ix] / (1.0 + X[:, Bathroom_ix]) # add 1 to avoid 0 division\n",
    "            return np.c_[X, rooms_per_building_area, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_building_area]\n",
    "\n",
    "num0_pipeline = Pipeline([\n",
    "        ('zeros2NaN',FunctionTransformer(func = replace_0_2_NaN,validate=False)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('log',FunctionTransformer(np.log1p, validate=True)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"constant\",fill_value='Unknown')),\n",
    "        ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "num_attribs0 = ['Landsize','BuildingArea']\n",
    "num_attribs1 = list(housing_num)\n",
    "cat_attribs = [\"CouncilArea\",'Type','Suburb','Postcode']\n",
    "\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num0\", num0_pipeline, num_attribs0),\n",
    "        (\"num1\", num_pipeline, num_attribs1),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing,housing_labels)\n",
    "housing_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferentes modelos de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- lin_reg ----\n",
      "Mean: 376591.4243453588\n",
      "Standard deviation: 39181.97082275836\n"
     ]
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "n_neighbors = 3\n",
    "knn_reg = neighbors.KNeighborsRegressor(n_neighbors)\n",
    "knn_reg.fit(housing_prepared, housing_labels)\n",
    "forest_reg = RandomForestRegressor(n_estimators=20, random_state=42)\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "svm_reg = SVR(kernel=\"linear\")\n",
    "svm_reg.fit(housing_prepared, housing_labels)\n",
    "models = [(lin_reg,\"lin_reg\")]\n",
    "for model in models:\n",
    "    scores = cross_val_score(model[0], housing_prepared, housing_labels, scoring=\"neg_root_mean_squared_error\", cv=5, n_jobs=-1)\n",
    "    display_scores(-scores, model[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mi pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared:  (4345, 22)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class DividedAtributes(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # no *args or **kargs\n",
    "        pass\n",
    "        #self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        proportion1 = X[:, 0] / (1.0 +X[:, 1])# add 1 to avoid 0 division\n",
    "        proportion2 = np.ones(proportion1.shape, dtype=np.float)/proportion1\n",
    "        return np.c_[proportion1,proportion1]\n",
    "\n",
    "    \n",
    "class invert_variable(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # no *args or **kargs\n",
    "        pass\n",
    "        #self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        data=np.zeros(X.shape,dtype=np.float)\n",
    "        for i in range(X.shape[1]):\n",
    "            data[:, 0]=1/X[:, 0]\n",
    "        return np.c_[X,data]\n",
    "class Categorical_2_mean(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # no *args or **kargs\n",
    "        self.medias_por_columnas=[]\n",
    "        self.media_y=0\n",
    "        #self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.media_y = y.mean()\n",
    "        self.medias_por_columnas=[]\n",
    "        #Para cada columna\n",
    "        for i in range(X.shape[1]):\n",
    "            medias = {}\n",
    "            columna = X[:,i]\n",
    "            unicos = np.unique(columna)\n",
    "            #Para cada valor guardo la media\n",
    "            for u in unicos:\n",
    "                medias[u]=y[columna==u].mean()\n",
    "                \n",
    "            self.medias_por_columnas.append(medias)\n",
    "            \n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        data = np.ones((X.shape[0]))\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            columna = X[:,i]\n",
    "            media = self.medias_por_columnas[i]\n",
    "            nueva_columna = np.zeros(X.shape[0])+self.media_y\n",
    "            \n",
    "            #Para cada valor distinto dentro de la categoría\n",
    "            for k in media.keys():\n",
    "                nueva_columna[columna == k] = media[k]\n",
    "            \n",
    "            #Si es la primera vez\n",
    "            if i == 0:\n",
    "                data = nueva_columna\n",
    "            else:\n",
    "                data = np.c_[data,nueva_columna]\n",
    "        return data\n",
    "\n",
    "class Clean_Outlayers_Quantile(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,q=0.01): # no *args or **kargs\n",
    "        self.q=q\n",
    "        self.low_q_col=[]\n",
    "        self.high_q_col=[]\n",
    "        #self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        #Para cada columna\n",
    "        for i in range(X.shape[1]):\n",
    "            columna = X[:,i]\n",
    "            self.high_q_col.append(np.quantile(a=columna , q=1-self.q))\n",
    "            self.low_q_col.append(np.quantile(a=columna ,q=self.q))\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        for i in range(X.shape[1]):\n",
    "            q_high=self.high_q_col[i]\n",
    "            q_low =self.low_q_col[i]\n",
    "            columna = X[:,i]\n",
    "            columna[columna>q_high]=q_high\n",
    "            columna[columna<q_low]=q_low\n",
    "        return X\n",
    "\n",
    "# Primero el preprocesamiento\n",
    "rf_pipe_num0 = Pipeline([\n",
    "    ('zeros2NaN',FunctionTransformer(func = replace_0_2_NaN,validate=False)),\n",
    "    (\"fill_nan\",SimpleImputer(strategy=\"mean\")),\n",
    "    (\"clean_outlayer\",Clean_Outlayers_Quantile()),\n",
    "    ('log',FunctionTransformer(np.log1p, validate=True)),\n",
    "    (\"std\",StandardScaler()),\n",
    "])\n",
    "\n",
    "rf_pipe_num1 = Pipeline([\n",
    "    (\"fill_nan\",SimpleImputer(strategy=\"mean\")),\n",
    "    (\"clean_outlayer\",Clean_Outlayers_Quantile()),\n",
    "    (\"std\",StandardScaler()),\n",
    "])\n",
    "\n",
    "rf_pipe_poli = Pipeline([\n",
    "    ('zeros2NaN',FunctionTransformer(func = replace_0_2_NaN,validate=False)),\n",
    "    (\"fill_nan\",SimpleImputer(strategy=\"mean\")),\n",
    "    ('zeros2NaN_2',FunctionTransformer(func = replace_0_2_NaN,validate=False)),# por si acaso hay algun 0\n",
    "    #(\"divided\",DividedAtributes()),\n",
    "    (\"invert_1/var\",invert_variable()),\n",
    "    (\"poly_interact_2\",PolynomialFeatures(interaction_only=True,degree=2)),\n",
    "    (\"std\",StandardScaler()),\n",
    "])\n",
    "\n",
    "rf_cat_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"constant\",fill_value='Unknown')),\n",
    "        ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "#cat2mean = Categorical_2_mean(minim_instances=10)\n",
    "rf_cat2mean_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"constant\",fill_value='Unknown')),\n",
    "        ('cat2mean', Categorical_2_mean()),\n",
    "        ('zeros2NaN',FunctionTransformer(func = replace_0_2_NaN,validate=False)),\n",
    "        (\"std\",StandardScaler()),\n",
    "])\n",
    "\"\"\"\n",
    "NUMERICAL:    ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea',\n",
    "'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "CATEGORICAL:  ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
    "\"\"\"\n",
    "num_attribs0 = ['Landsize','BuildingArea']\n",
    "num_attribs1 = ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car',\n",
    "'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "\n",
    "num_attribs_poli = ['Rooms','BuildingArea']\n",
    "#list(housing_num)\n",
    "cat_attribs = []#'Type',\"CouncilArea\",'Suburb','Regionname','Postcode']\n",
    "\n",
    "rf_full_pipe = ColumnTransformer([\n",
    "    (\"num0\", rf_pipe_num0, num_attribs0),\n",
    "    (\"num1\", rf_pipe_num1, num_attribs1),\n",
    "    (\"poli\", rf_pipe_poli, num_attribs_poli),\n",
    "    (\"cat\",  rf_cat_pipe, cat_attribs),\n",
    "    (\"cat_2_mean\",  rf_cat2mean_pipe, cat_attribs)\n",
    "])\n",
    "\n",
    "X_train_prepared = rf_full_pipe.fit_transform(X_train,y_train)\n",
    "\n",
    "X_test_prepared = rf_full_pipe.transform(test_housing)\n",
    "print(\"Prepared: \",X_train_prepared.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1 = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "                         scoring=\"neg_root_mean_squared_error\", cv=10,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El base: \n",
      "Mean: 417320.9395217064\n",
      "Standard deviation: 31111.63494074384\n",
      "El mío: \n",
      "Mean: 443484.79720145726\n",
      "Standard deviation: 42876.75489063647\n"
     ]
    }
   ],
   "source": [
    "housing_prepared_mio = X_train_prepared\n",
    "\n",
    "scores2 = cross_val_score(tree_reg, housing_prepared_mio, housing_labels,\n",
    "                         scoring=\"neg_root_mean_squared_error\", cv=10,n_jobs=-1)\n",
    "print(\"El base: \")\n",
    "display_scores(-scores1)\n",
    "print(\"El mío: \")\n",
    "display_scores(-scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEl base: \\nMean: 417320.9395217064\\nStandard deviation: 31111.63494074384\\nEl mío: \\nMean: 403739.0195635212\\nStandard deviation: 40121.749009224426\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "El base: \n",
    "Mean: 417320.9395217064\n",
    "Standard deviation: 31111.63494074384\n",
    "El mío: \n",
    "Mean: 403739.0195635212\n",
    "Standard deviation: 40121.749009224426\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared = full_pipeline.fit_transform(housing,housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 . Score:  365278.50828061753\n",
      "Tiempo (s) : 0.663611888885498\n",
      "5  Att.  365278.50828061753 {'num_log_att': ['Landsize', 'BuildingArea'], 'num_att': ['Rooms', 'Distance', 'Bathroom', 'Car', 'YearBuilt', 'Lattitude'], 'num_poli_att': ['Rooms', 'BuildingArea'], 'cat_att': ['Type', 'Regionname', 'Postcode'], 'cat2mean_att': ['Type', 'CouncilArea', 'Suburb', 'Regionname', 'Postcode']}\n",
      "{'num_log_att': ['Landsize', 'BuildingArea'], 'num_att': ['Rooms', 'Distance', 'Bathroom', 'Car', 'YearBuilt', 'Lattitude'], 'num_poli_att': ['Rooms', 'BuildingArea'], 'cat_att': ['Type', 'Regionname', 'Postcode'], 'cat2mean_att': ['Type', 'CouncilArea', 'Suburb', 'Regionname', 'Postcode']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "NUMERICAL:    ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea',\n",
    "'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "CATEGORICAL:  ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
    "\"\"\"\n",
    "num_log_att = ['Landsize','BuildingArea']\n",
    "num_att = ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car',\n",
    "'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "\n",
    "num_poli_att = ['Rooms','BuildingArea','Bathroom']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#list(housing_num)\n",
    "cat_att = ['Type',\"CouncilArea\",'Suburb','Regionname','Postcode']\n",
    "cat2mean_att = ['Type',\"CouncilArea\",'Suburb','Regionname','Postcode']\n",
    "\n",
    "initial_atr = {\n",
    "    \"num_log_att\":num_log_att.copy() ,\n",
    "    \"num_att\":num_att.copy() ,\n",
    "    \"num_poli_att\":num_poli_att.copy() ,\n",
    "    \"cat_att\":cat_att.copy() ,\n",
    "    \"cat2mean_att\":cat2mean_att.copy() ,\n",
    "}\n",
    "\n",
    "initial_atr = {'num_log_att': ['Landsize', 'BuildingArea'],\n",
    "               'num_att': ['Rooms', 'Distance', 'Bathroom', 'Car', 'YearBuilt', 'Lattitude'],\n",
    "               'num_poli_att': ['Rooms', 'BuildingArea'], \n",
    "               'cat_att': ['Type', 'Regionname', 'Postcode'], \n",
    "               'cat2mean_att': ['Type', 'CouncilArea', 'Suburb', 'Regionname', 'Postcode']\n",
    "              }\n",
    "\n",
    "initial_full_pipe = ColumnTransformer([\n",
    "            (\"num0\", rf_pipe_num0, initial_atr[\"num_log_att\"]),\n",
    "            (\"num1\", rf_pipe_num1, initial_atr[\"num_att\"]),\n",
    "            (\"poli\", rf_pipe_poli, initial_atr[\"num_poli_att\"]),\n",
    "            (\"cat\",  rf_cat_pipe, initial_atr[\"cat_att\"]),\n",
    "            (\"cat_2_mean\",rf_cat2mean_pipe, initial_atr[\"cat2mean_att\"])\n",
    "])\n",
    "\n",
    "\n",
    "#model = RandomForestRegressor(random_state=42,n_estimators=30,max_features=\"sqrt\",max_depth=20)\n",
    "model = LinearRegression()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "X_train_initial = initial_full_pipe.fit_transform(X_train,y_train)\n",
    "previous_score_winner = -cross_val_score(model, X_train_initial, housing_labels,scoring=\"neg_root_mean_squared_error\", cv=5).mean()\n",
    "\n",
    "actual_list_atts = []    \n",
    "for k in initial_atr.keys():\n",
    "    for v in initial_atr[k]:\n",
    "        actual_list_atts.append((k,v))\n",
    "\n",
    "\n",
    "def create_att_dic(list_att):\n",
    "    dic_att={}\n",
    "    for key in initial_atr.keys():\n",
    "        dic_att[key]=[]\n",
    "    for (k,v) in list_att:\n",
    "        dic_att[k].append(v)\n",
    "    return dic_att\n",
    "        \n",
    "\n",
    "buscando = True\n",
    "\n",
    "\n",
    "cont=0\n",
    "while buscando and len(actual_list_atts)!=0:\n",
    "    \n",
    "    end = time.time()\n",
    "    tiempo=end - start\n",
    "    start = end\n",
    "    \n",
    "    dic_=create_att_dic(actual_list_atts)\n",
    "    print(cont,\". Score: \",previous_score_winner)\n",
    "    print(\"Tiempo (s) :\" ,tiempo )\n",
    "    print(len(dic_),\" Att. \",previous_score_winner, dic_)\n",
    "\n",
    "    scores_ronda=[]\n",
    "    for at in actual_list_atts:\n",
    "        actual_list_without_at = actual_list_atts.copy()\n",
    "        actual_list_without_at.remove(at)\n",
    "        #Aqui hemos quitado la feature\n",
    "        dict_att=create_att_dic(actual_list_without_at)\n",
    "        #Creamos la nueva pipe sin dicho atributo\n",
    "        actual_full_pipe = ColumnTransformer([\n",
    "            (\"num0\", rf_pipe_num0, dict_att[\"num_log_att\"]),\n",
    "            (\"num1\", rf_pipe_num1, dict_att[\"num_att\"]),\n",
    "            (\"poli\", rf_pipe_poli, dict_att[\"num_poli_att\"]),\n",
    "            (\"cat\",  rf_cat_pipe, dict_att[\"cat_att\"]),\n",
    "            (\"cat_2_mean\",rf_cat2mean_pipe, dict_att[\"cat2mean_att\"])\n",
    "        ])\n",
    "        #Procesamos los datos con la nueva pipe\n",
    "        X_train_actual = actual_full_pipe.fit_transform(X_train,y_train)\n",
    "        #Calculamos con el modelo y guardamos como de bueno es\n",
    "        actual_score = - cross_val_score(model, X_train_actual, housing_labels,scoring=\"neg_root_mean_squared_error\", cv=5, n_jobs=-1).mean()\n",
    "        scores_ronda.append((actual_score,at))\n",
    "    \n",
    "    winner = min(scores_ronda, key= lambda x : x[0])\n",
    "    #Aquí tenemos el mejor score y el atributo que hay que quitar para tenerlo\n",
    "    score_winner, att_winner = winner\n",
    "    \n",
    "    #Si este resultado es mejor que el mejor de la ronda anterior\n",
    "    if score_winner < previous_score_winner:\n",
    "        actual_list_atts.remove(att_winner)\n",
    "        previous_score_winner = score_winner\n",
    "        \n",
    "    #Si no es mejor hemos acabado\n",
    "    else:\n",
    "        buscando=False\n",
    "    \n",
    "final_att = create_att_dic(actual_list_atts)\n",
    "print(final_att)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
