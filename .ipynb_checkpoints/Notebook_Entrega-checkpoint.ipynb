{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primero de todo los imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "\n",
    "# Creamos un directiorio para la salida\n",
    "import os\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "\n",
    "\n",
    "\n",
    "# Para hacer test y train\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Para ajustar los hiper-parámetros\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Preproceso\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Función de error\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to display score results from CV\n",
    "def display_scores(scores,model_name = None):\n",
    "    if(model_name):\n",
    "        print(\"----\",model_name,\"----\")\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "    \n",
    "def apply_gridsearch(model,params,X_train_gs,y_train_gs):\n",
    "    grid_search = GridSearchCV(model, param_grid=params, cv=5,\n",
    "                               scoring='neg_root_mean_squared_error',\n",
    "                               return_train_score=True,n_jobs=-1)\n",
    "    grid_search.fit(X_train_gs, y_train_gs)\n",
    "\n",
    "    print(\"Best trained model:\")\n",
    "    print(grid_search.best_estimator_)\n",
    "    print(\"Best parameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(\"Best Score\")\n",
    "    print(np.sqrt(-grid_search.best_score_),-grid_search.best_score_)\n",
    "    \n",
    "def create_att_dic(list_att):\n",
    "    dic_att={}\n",
    "    for key in initial_atr.keys():\n",
    "        dic_att[key]=[]\n",
    "    for (k,v) in list_att:\n",
    "        dic_att[k].append(v)\n",
    "    return dic_att\n",
    "\n",
    "def feature_selection(model, dic_):\n",
    "    num_log_att = ['Landsize','BuildingArea']\n",
    "    num_att = ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car','YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "    num_poli_att = ['Rooms','BuildingArea','Bathroom']\n",
    "    #list(housing_num)\n",
    "    cat_att = ['Type',\"CouncilArea\",'Suburb','Regionname','Postcode']\n",
    "    cat2mean_att = ['Type',\"CouncilArea\",'Suburb','Regionname','Postcode']\n",
    "    \n",
    "    \n",
    "    initial_atr = {\n",
    "        \"num_log_att\":num_log_att.copy() ,\n",
    "        \"num_att\":num_att.copy() ,\n",
    "        \"num_poli_att\":num_poli_att.copy() ,\n",
    "        \"cat_att\":cat_att.copy() ,\n",
    "        \"cat2mean_att\":cat2mean_att.copy() ,\n",
    "    }\n",
    "\n",
    "    initial_atr = {'num_log_att': ['Landsize', 'BuildingArea'],\n",
    "                   'num_att': ['Rooms', 'Distance', 'Bathroom', 'Car', 'YearBuilt', 'Lattitude'],\n",
    "                   'num_poli_att': ['Rooms', 'BuildingArea'], \n",
    "                   'cat_att': ['Type', 'Regionname', 'Postcode'], \n",
    "                   'cat2mean_att': ['Type', 'CouncilArea', 'Suburb', 'Regionname', 'Postcode']\n",
    "                  }\n",
    "\n",
    "    initial_full_pipe = ColumnTransformer([\n",
    "                (\"num0\", rf_pipe_num0, initial_atr[\"num_log_att\"]),\n",
    "                (\"num1\", rf_pipe_num1, initial_atr[\"num_att\"]),\n",
    "                (\"poli\", rf_pipe_poli, initial_atr[\"num_poli_att\"]),\n",
    "                (\"cat\",  rf_cat_pipe, initial_atr[\"cat_att\"]),\n",
    "                (\"cat_2_mean\",rf_cat2mean_pipe, initial_atr[\"cat2mean_att\"])\n",
    "    ])\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    X_train_initial = initial_full_pipe.fit_transform(X_train,y_train)\n",
    "    previous_score_winner = -cross_val_score(model, X_train_initial, housing_labels,scoring=\"neg_root_mean_squared_error\", cv=5).mean()\n",
    "\n",
    "    actual_list_atts = []    \n",
    "    for k in initial_atr.keys():\n",
    "        for v in initial_atr[k]:\n",
    "            actual_list_atts.append((k,v))\n",
    "    \n",
    "    buscando = True\n",
    "    cont=0\n",
    "    while buscando and len(actual_list_atts)!=0:\n",
    "\n",
    "        end = time.time()\n",
    "        tiempo=end - start\n",
    "        start = end\n",
    "\n",
    "        dic_=create_att_dic(actual_list_atts)\n",
    "        print(cont,\". Score: \",previous_score_winner)\n",
    "        print(\"Tiempo (s) :\" ,tiempo )\n",
    "        print(len(dic_),\" Att. \",previous_score_winner, dic_)\n",
    "\n",
    "        scores_ronda=[]\n",
    "        for at in actual_list_atts:\n",
    "            actual_list_without_at = actual_list_atts.copy()\n",
    "            actual_list_without_at.remove(at)\n",
    "            #Aqui hemos quitado la feature\n",
    "            dict_att=create_att_dic(actual_list_without_at)\n",
    "            #Creamos la nueva pipe sin dicho atributo\n",
    "            actual_full_pipe = ColumnTransformer([\n",
    "                (\"num0\", rf_pipe_num0, dict_att[\"num_log_att\"]),\n",
    "                (\"num1\", rf_pipe_num1, dict_att[\"num_att\"]),\n",
    "                (\"poli\", rf_pipe_poli, dict_att[\"num_poli_att\"]),\n",
    "                (\"cat\",  rf_cat_pipe, dict_att[\"cat_att\"]),\n",
    "                (\"cat_2_mean\",rf_cat2mean_pipe, dict_att[\"cat2mean_att\"])\n",
    "            ])\n",
    "            #Procesamos los datos con la nueva pipe\n",
    "            X_train_actual = actual_full_pipe.fit_transform(X_train,y_train)\n",
    "            #Calculamos con el modelo y guardamos como de bueno es\n",
    "            actual_score = - cross_val_score(model, X_train_actual, housing_labels,scoring=\"neg_root_mean_squared_error\", cv=5, n_jobs=-1).mean()\n",
    "            scores_ronda.append((actual_score,at))\n",
    "\n",
    "        winner = min(scores_ronda, key= lambda x : x[0])\n",
    "        #Aquí tenemos el mejor score y el atributo que hay que quitar para tenerlo\n",
    "        score_winner, att_winner = winner\n",
    "\n",
    "        #Si este resultado es mejor que el mejor de la ronda anterior\n",
    "        if score_winner < previous_score_winner:\n",
    "            actual_list_atts.remove(att_winner)\n",
    "            previous_score_winner = score_winner\n",
    "\n",
    "        #Si no es mejor hemos acabado\n",
    "        else:\n",
    "            buscando=False\n",
    "\n",
    "    final_att = create_att_dic(actual_list_atts)\n",
    "    print(\"ACABADO: \")\n",
    "    print(final_att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de casas: 5432  Número de características: 21\n",
      "\n",
      " Características numéricas: \n",
      "  ['Rooms', 'Price', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
      "Características categóricas: \n",
      " ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el dataset\n",
    "housing = pd.read_csv('dataset/housing-snapshot/train_set.csv',index_col=0) \n",
    "print(\"Número de casas:\",housing.shape[0],\" Número de características:\", housing.shape[1])\n",
    "housing_num = housing.select_dtypes(exclude=[np.object]).columns\n",
    "housing_cat = housing.select_dtypes(include=[np.object]).columns\n",
    "print(\"\\n Características numéricas: \\n \", list(housing_num))\n",
    "\n",
    "print(\"Características categóricas: \\n\", list(housing_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ponemos \"Postcode\" a categorical, dado que que no es numérica.\n",
    "housing['Postcode'] = pd.Categorical(housing.Postcode)\n",
    "\n",
    "# Dividimos haciendo uso de la estratificación para tener una buena proporción.\n",
    "housing[\"price_aux\"] = pd.cut(housing[\"Price\"],\n",
    "                               bins=[0., 500000, 1000000, 1500000, 2000000., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"price_aux\"]):\n",
    "    train_set = housing.loc[train_index]\n",
    "    test_set = housing.loc[test_index]\n",
    "\n",
    "# Quitamos la etiqueta price_cat\n",
    "train_set.drop(\"price_aux\", axis=1, inplace=True)\n",
    "test_set.drop(\"price_aux\", axis=1, inplace=True)\n",
    "housing.drop(\"price_aux\", axis=1, inplace=True)\n",
    "\n",
    "# El con el train probaremos los modelos.\n",
    "X_train = train_set.drop(\"Price\", axis=1).copy()\n",
    "y_train = train_set[\"Price\"].copy()\n",
    "# El test no lo tenemos que tocar hasta el final de todo.\n",
    "X_test = test_set.drop(\"Price\", axis=1).copy()\n",
    "y_test = test_set[\"Price\"].copy()\n",
    "\n",
    "\n",
    "housing_num = X_train.select_dtypes(exclude=[np.object]).columns\n",
    "housing_cat = X_train.select_dtypes(include=[np.object]).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Las funciones de las pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a function to replace 0 by NaN\n",
    "def replace_0_2_NaN(data):\n",
    "    data[data == 0] = np.nan\n",
    "    return data\n",
    "\n",
    "\n",
    "# column index\n",
    "Rooms_ix, Bedroom2_ix, Bathroom_ix, BuildingArea_ix = 0, 2, 3, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        rooms_per_building_area = X[:, Rooms_ix] / (1.0 +X[:, BuildingArea_ix])# add 1 to avoid 0 division\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, Bedroom2_ix] / (1.0 + X[:, Bathroom_ix]) # add 1 to avoid 0 division\n",
    "            return np.c_[X, rooms_per_building_area, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_building_area]\n",
    "\n",
    "\n",
    "class DividedAtributes(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # no *args or **kargs\n",
    "        pass\n",
    "        #self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        proportion1 = X[:, 0] / (1.0 +X[:, 1])# add 1 to avoid 0 division\n",
    "        proportion2 = np.ones(proportion1.shape, dtype=np.float)/proportion1\n",
    "        return np.c_[proportion1,proportion1]\n",
    "\n",
    "    \n",
    "class invert_variable(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # no *args or **kargs\n",
    "        pass\n",
    "        #self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        data=np.zeros(X.shape,dtype=np.float)\n",
    "        for i in range(X.shape[1]):\n",
    "            data[:, 0]=1/X[:, 0]\n",
    "        return np.c_[X,data]\n",
    "class Categorical_2_mean(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # no *args or **kargs\n",
    "        self.medias_por_columnas=[]\n",
    "        self.media_y=0\n",
    "        #self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.media_y = y.mean()\n",
    "        self.medias_por_columnas=[]\n",
    "        #Para cada columna\n",
    "        for i in range(X.shape[1]):\n",
    "            medias = {}\n",
    "            columna = X[:,i]\n",
    "            unicos = np.unique(columna)\n",
    "            #Para cada valor guardo la media\n",
    "            for u in unicos:\n",
    "                medias[u]=y[columna==u].mean()\n",
    "                \n",
    "            self.medias_por_columnas.append(medias)\n",
    "            \n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        data = np.ones((X.shape[0]))\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            columna = X[:,i]\n",
    "            media = self.medias_por_columnas[i]\n",
    "            nueva_columna = np.zeros(X.shape[0])+self.media_y\n",
    "            \n",
    "            #Para cada valor distinto dentro de la categoría\n",
    "            for k in media.keys():\n",
    "                nueva_columna[columna == k] = media[k]\n",
    "            \n",
    "            #Si es la primera vez\n",
    "            if i == 0:\n",
    "                data = nueva_columna\n",
    "            else:\n",
    "                data = np.c_[data,nueva_columna]\n",
    "        return data\n",
    "\n",
    "class Clean_Outlayers_Quantile(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,q=0.01): # no *args or **kargs\n",
    "        self.q=q\n",
    "        self.low_q_col=[]\n",
    "        self.high_q_col=[]\n",
    "        #self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        #Para cada columna\n",
    "        for i in range(X.shape[1]):\n",
    "            columna = X[:,i]\n",
    "            self.high_q_col.append(np.quantile(a=columna , q=1-self.q))\n",
    "            self.low_q_col.append(np.quantile(a=columna ,q=self.q))\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        for i in range(X.shape[1]):\n",
    "            q_high=self.high_q_col[i]\n",
    "            q_low =self.low_q_col[i]\n",
    "            columna = X[:,i]\n",
    "            columna[columna>q_high]=q_high\n",
    "            columna[columna<q_low]=q_low\n",
    "        return X    \n",
    "\n",
    "# Ejemplo de PipeLines\n",
    "\"\"\"\n",
    "1.Para las columnas con muchos ceros sin sentido. \n",
    "Además se les aplicará la función logaritmo.\n",
    "Las columnas que tienen sentido en esta pipe son \"BuildingArea\" y \"Landsize\" \n",
    "\"\"\"\n",
    "num0_pipeline = Pipeline([\n",
    "        ('zeros2NaN',FunctionTransformer(func = replace_0_2_NaN,validate=False)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('log',FunctionTransformer(np.log1p, validate=True)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# 2.Para las otras columnas numéricas\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Para las variables categóricas. Principalmente utilizaremos el OneHotEncoder\n",
    "Esta pipe convierte columnas categóricas en numéricas creando una columna de \n",
    "1's y 0's por cada valor único en la columna original.\n",
    "\n",
    "Ejemplo:\n",
    "Original \"Type\":\n",
    "Type\n",
    "h\n",
    "t\n",
    "h\n",
    "u\n",
    "h\n",
    "\n",
    "Dummies from \"Type\":\n",
    "h t u\n",
    "1 0 0\n",
    "0 1 0\n",
    "1 0 0\n",
    "0 0 1\n",
    "1 0 0\n",
    "\n",
    "Esto sube muchísimo la dimensionalidad si la columna original\n",
    "tiene muchos valores distintos, así que hay que tratarlo con cuidado.\n",
    "\n",
    "\"\"\"\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"constant\",fill_value='Unknown')),\n",
    "        ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "num_attribs0 = ['Landsize','BuildingArea']\n",
    "num_attribs1 = list(housing_num)\n",
    "cat_attribs = [\"CouncilArea\",'Type','Suburb','Postcode']\n",
    "# Esta es una especie de Pipeline madre que coge otras pipelines y procesa\n",
    "# todo el data set. Cada pipeline se aplica a las columnas de la lista\n",
    "# num0_pipeline con las columnas en la lista num_attribs0\n",
    "full_pipeline_ejemplo = ColumnTransformer([\n",
    "        (\"num0\", num0_pipeline, num_attribs0),\n",
    "        (\"num1\", num_pipeline, num_attribs1),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los modelos\n",
    "\n",
    "Vamos a tratar unos modelos de machine learning y en concreto haremos uso de la famosa librería Sklearn. En cada uno de los distintos modelos vamos a mostrar primero una implementación naíf y luego vamos a hacer un preproceso de datos personalizado, una feature selection y cuando sea posible una optimización de los hiper-parámetros para conseguir los mejores resultados posibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn import neighbors    \n",
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación naíf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- lin_reg ----\n",
      "Mean: 384025.51472573285\n",
      "Standard deviation: 48252.21371717167\n",
      "---- KNN-Regressor ----\n",
      "Mean: 375914.6862240796\n",
      "Standard deviation: 31220.851831198084\n",
      "---- Random Forest ----\n",
      "Mean: 310410.0826121716\n",
      "Standard deviation: 36235.270271245245\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Para las otras columnas numéricas\n",
    "num_naif_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "#Para las variables categóricas. Principalmente utilizaremos el OneHotEncoder\n",
    "cat_naif_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"constant\",fill_value='Unknown')),\n",
    "        ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\"\"\"\n",
    "Columnas numéricas:  ['Rooms', 'Price', 'Distance', 'Bedroom2', 'Bathroom', 'Car', \n",
    "'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "\n",
    "Columnas categóricas:  ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
    "\"\"\"\n",
    "\n",
    "#Estas son las columnas seleccionadas en el proyecto base para cada pipeline.\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"CouncilArea\",'Type','Suburb','Postcode']\n",
    "\n",
    "#Creamos la \"full_pipeline\", es decir la pipeline que engloba a todas las otras.\n",
    "full_naif_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_naif_pipeline, num_attribs),\n",
    "        (\"cat\", cat_naif_pipeline, cat_attribs),\n",
    "])\n",
    "#Preprocesamos los datos utilizando la \"full_pipeline\"\n",
    "housing_prepared = full_naif_pipeline.fit_transform(X_train,y_train)\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "forest_reg.fit(housing_prepared, y_train)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, y_train)\n",
    "\n",
    "\n",
    "n_neighbors = 3\n",
    "knn_reg = neighbors.KNeighborsRegressor(n_neighbors)\n",
    "knn_reg.fit(housing_prepared, y_train)\n",
    "\n",
    "models = [(lin_reg,\"lin_reg\"),\n",
    "          (knn_reg,\"KNN-Regressor\"),\n",
    "          (forest_reg,'Random Forest')\n",
    "]\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    scores = cross_val_score(model[0], housing_prepared, y_train, scoring=\"neg_root_mean_squared_error\", cv=5,n_jobs=-1)\n",
    "    display_scores(-scores, model[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarda mucho en ejecutarse (Si quieres ejecutarlo simplemente descomenta el for que está justo al final de la celda anterior.), así que aquí pongo los resultados:\n",
    "\n",
    "    -lin_reg:\n",
    "        Mean: 384025.51472573285\n",
    "        Standard deviation: 48252.21371717167\n",
    "    -KNN-Regressor:\n",
    "        Mean: 375914.6862240796\n",
    "        Standard deviation: 31220.851831198084\n",
    "    -Random Forest ----\n",
    "        Mean: 310410.0826121716\n",
    "        Standard deviation: 36235.270271245245\n",
    "        \n",
    "Estos serán nuestros resultados base, nuestro punto de referencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset original: (4345, 20)\n",
      "Dataset procesado: (4345, 8)\n"
     ]
    }
   ],
   "source": [
    "X_train_knn = X_train.copy()\n",
    "y_train_knn = y_train.copy()\n",
    "\n",
    "\n",
    "\n",
    "num_pipe_log  = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        (\"clean_outlayer\",Clean_Outlayers_Quantile()),\n",
    "        ('log',FunctionTransformer(np.log1p, validate=True)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "num_pipe      = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        (\"clean_outlayer\",Clean_Outlayers_Quantile()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"constant\",fill_value='Unknown')),\n",
    "        ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "# Características seleccionadas:\n",
    "num_features = ['Rooms',\"Bathroom\",\"Lattitude\",\"Longtitude\"]\n",
    "log_features = ['Landsize']\n",
    "cat_features = ['Type']\n",
    "\n",
    "full_pipe_knn = ColumnTransformer([\n",
    "        (\"num_log\", num_pipe_log, log_features),\n",
    "        (\"num\", num_pipe, num_features),\n",
    "        (\"cat\", cat_pipe, cat_features),\n",
    "])\n",
    "\n",
    "X_train_knn_prepared=full_pipe_knn.fit_transform(X_train_knn,y_train_knn)\n",
    "print(\"Dataset original:\",X_train_knn.shape)\n",
    "print(\"Dataset procesado:\",X_train_knn_prepared.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- KNeighborsRegressor(n_neighbors=3) ----\n",
      "Mean: 358312.70383498963\n",
      "Standard deviation: 23930.486517285564\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "scores = cross_val_score(knn_reg, X_train_knn_prepared, y_train_knn, scoring=\"neg_root_mean_squared_error\", cv=5,n_jobs=-1)\n",
    "display_scores(-scores, knn_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    KNeighborsRegressor(n_neighbors=3)\n",
    "        -Mean: 358312.70383498963\n",
    "        -Standard deviation: 23930.486517285564"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creemos que estos resultados se pueden mejorar un poco más haciendo uso de la función GridSearch CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trained model:\n",
      "KNeighborsRegressor()\n",
      "Best parameters:\n",
      "{'n_neighbors': 5}\n",
      "Best Score\n",
      "595.4468825205093 354556.9899033933\n"
     ]
    }
   ],
   "source": [
    "knn_param_grid={ \"n_neighbors\": range(1,20,1)\n",
    "    \n",
    "}\n",
    "\n",
    "apply_gridsearch(model=knn_reg,\n",
    "                 params=knn_param_grid,\n",
    "                 X_train_gs=X_train_knn_prepared,\n",
    "                 y_train_gs=y_train_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regresión Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lr = X_train.copy()\n",
    "y_train_lr = y_train.copy()\n",
    "# X_train_lr,y_train_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero el preprocesamiento\n",
    "rf_pipe_num0 = Pipeline([\n",
    "    ('zeros2NaN',FunctionTransformer(func = replace_0_2_NaN,validate=False)),\n",
    "    (\"fill_nan\",SimpleImputer(strategy=\"mean\")),\n",
    "    (\"clean_outlayer\",Clean_Outlayers_Quantile()),\n",
    "    ('log',FunctionTransformer(np.log1p, validate=True)),\n",
    "    (\"std\",StandardScaler()),\n",
    "])\n",
    "\n",
    "rf_pipe_num1 = Pipeline([\n",
    "    (\"fill_nan\",SimpleImputer(strategy=\"mean\")),\n",
    "    (\"clean_outlayer\",Clean_Outlayers_Quantile()),\n",
    "    (\"std\",StandardScaler()),\n",
    "])\n",
    "\n",
    "rf_pipe_poli = Pipeline([\n",
    "    ('zeros2NaN',FunctionTransformer(func = replace_0_2_NaN,validate=False)),\n",
    "    (\"fill_nan\",SimpleImputer(strategy=\"mean\")),\n",
    "    ('zeros2NaN_2',FunctionTransformer(func = replace_0_2_NaN,validate=False)),# por si acaso hay algun 0\n",
    "    #(\"divided\",DividedAtributes()),\n",
    "    (\"invert_1/var\",invert_variable()),\n",
    "    (\"poly_interact_2\",PolynomialFeatures(interaction_only=True,degree=2)),\n",
    "    (\"std\",StandardScaler()),\n",
    "])\n",
    "\n",
    "rf_cat_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"constant\",fill_value='Unknown')),\n",
    "        ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "#cat2mean = Categorical_2_mean(minim_instances=10)\n",
    "rf_cat2mean_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"constant\",fill_value='Unknown')),\n",
    "        ('cat2mean', Categorical_2_mean()),\n",
    "        ('zeros2NaN',FunctionTransformer(func = replace_0_2_NaN,validate=False)),\n",
    "        (\"std\",StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 . Score:  365278.50828061753\n",
      "Tiempo (s) : 0.35442090034484863\n",
      "5  Att.  365278.50828061753 {'num_log_att': ['Landsize', 'BuildingArea'], 'num_att': ['Rooms', 'Distance', 'Bathroom', 'Car', 'YearBuilt', 'Lattitude'], 'num_poli_att': ['Rooms', 'BuildingArea'], 'cat_att': ['Type', 'Regionname', 'Postcode'], 'cat2mean_att': ['Type', 'CouncilArea', 'Suburb', 'Regionname', 'Postcode']}\n",
      "ACABADO: \n",
      "{'num_log_att': ['Landsize', 'BuildingArea'], 'num_att': ['Rooms', 'Distance', 'Bathroom', 'Car', 'YearBuilt', 'Lattitude'], 'num_poli_att': ['Rooms', 'BuildingArea'], 'cat_att': ['Type', 'Regionname', 'Postcode'], 'cat2mean_att': ['Type', 'CouncilArea', 'Suburb', 'Regionname', 'Postcode']}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NUMERICAL:    ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea',\n",
    "'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "CATEGORICAL:  ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
    "\"\"\"\n",
    "num_log_att = ['Landsize','BuildingArea']\n",
    "num_att = ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car',\n",
    "'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "num_poli_att = ['Rooms','BuildingArea','Bathroom']\n",
    "\n",
    "cat_att = ['Type',\"CouncilArea\",'Suburb','Regionname','Postcode']\n",
    "cat2mean_att = ['Type',\"CouncilArea\",'Suburb','Regionname','Postcode']\n",
    "\n",
    "initial_atr = {\n",
    "    \"num_log_att\":num_log_att.copy() ,\n",
    "    \"num_att\":num_att.copy() ,\n",
    "    \"num_poli_att\":num_poli_att.copy() ,\n",
    "    \"cat_att\":cat_att.copy() ,\n",
    "    \"cat2mean_att\":cat2mean_att.copy() ,\n",
    "}\n",
    "\"\"\"\n",
    "# Descomentar esto si no se quiere esperar a que se ejecute toda la feature selection.\n",
    "initial_atr = {\n",
    "                'num_log_att': ['Landsize', 'BuildingArea'], \n",
    "                'num_att': ['Rooms', 'Distance', 'Bathroom', 'Car', 'YearBuilt', 'Lattitude'], \n",
    "                'num_poli_att': ['Rooms', 'BuildingArea'], 'cat_att': ['Type', 'Regionname', 'Postcode'], \n",
    "                'cat2mean_att': ['Type', 'CouncilArea', 'Suburb', 'Regionname', 'Postcode']\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "initial_full_pipe = ColumnTransformer([\n",
    "            (\"num0\", rf_pipe_num0, initial_atr[\"num_log_att\"]),\n",
    "            (\"num1\", rf_pipe_num1, initial_atr[\"num_att\"]),\n",
    "            (\"poli\", rf_pipe_poli, initial_atr[\"num_poli_att\"]),\n",
    "            (\"cat\",  rf_cat_pipe, initial_atr[\"cat_att\"]),\n",
    "            (\"cat_2_mean\",rf_cat2mean_pipe, initial_atr[\"cat2mean_att\"])\n",
    "])\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "X_train_initial = initial_full_pipe.fit_transform(X_train_lr,y_train_lr)\n",
    "previous_score_winner = -cross_val_score(model, X_train_initial, y_train_lr,scoring=\"neg_root_mean_squared_error\", cv=5,n_jobs=-1).mean()\n",
    "\n",
    "actual_list_atts = []    \n",
    "for k in initial_atr.keys():\n",
    "    for v in initial_atr[k]:\n",
    "        actual_list_atts.append((k,v))\n",
    "\n",
    "\n",
    "def create_att_dic(list_att):\n",
    "    dic_att={}\n",
    "    for key in initial_atr.keys():\n",
    "        dic_att[key]=[]\n",
    "    for (k,v) in list_att:\n",
    "        dic_att[k].append(v)\n",
    "    return dic_att\n",
    "        \n",
    "\n",
    "buscando = True\n",
    "cont=0\n",
    "while buscando and len(actual_list_atts)!=0:\n",
    "    \n",
    "    end = time.time()\n",
    "    tiempo=end - start\n",
    "    start = end\n",
    "    \n",
    "    dic_=create_att_dic(actual_list_atts)\n",
    "    print(cont,\". Score: \",previous_score_winner)\n",
    "    print(\"Tiempo (s) :\" ,tiempo )\n",
    "    print(len(dic_),\" Att. \",previous_score_winner, dic_)\n",
    "\n",
    "    scores_ronda=[]\n",
    "    for at in actual_list_atts:\n",
    "        actual_list_without_at = actual_list_atts.copy()\n",
    "        actual_list_without_at.remove(at)\n",
    "        #Aqui hemos quitado la feature\n",
    "        dict_att=create_att_dic(actual_list_without_at)\n",
    "        #Creamos la nueva pipe sin dicho atributo\n",
    "        actual_full_pipe = ColumnTransformer([\n",
    "            (\"num0\", rf_pipe_num0, dict_att[\"num_log_att\"]),\n",
    "            (\"num1\", rf_pipe_num1, dict_att[\"num_att\"]),\n",
    "            (\"poli\", rf_pipe_poli, dict_att[\"num_poli_att\"]),\n",
    "            (\"cat\",  rf_cat_pipe, dict_att[\"cat_att\"]),\n",
    "            (\"cat_2_mean\",rf_cat2mean_pipe, dict_att[\"cat2mean_att\"])\n",
    "        ])\n",
    "        #Procesamos los datos con la nueva pipe\n",
    "        X_train_actual = actual_full_pipe.fit_transform(X_train_lr,y_train_lr)\n",
    "        #Calculamos con el modelo y guardamos como de bueno es\n",
    "        actual_score = - cross_val_score(model, X_train_actual, y_train_lr,scoring=\"neg_root_mean_squared_error\", cv=5, n_jobs=-1).mean()\n",
    "        scores_ronda.append((actual_score,at))\n",
    "    \n",
    "    winner = min(scores_ronda, key= lambda x : x[0])\n",
    "    #Aquí tenemos el mejor score y el atributo que hay que quitar para tenerlo\n",
    "    score_winner, att_winner = winner\n",
    "    \n",
    "    #Si este resultado es mejor que el mejor de la ronda anterior\n",
    "    if score_winner < previous_score_winner:\n",
    "        actual_list_atts.remove(att_winner)\n",
    "        previous_score_winner = score_winner\n",
    "        \n",
    "    #Si no es mejor hemos acabado\n",
    "    else:\n",
    "        buscando=False\n",
    "    \n",
    "final_att = create_att_dic(actual_list_atts)\n",
    "print(\"ACABADO: \")\n",
    "print(final_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "atr_final_lr = {\n",
    "                'num_log_att': ['Landsize', 'BuildingArea'], \n",
    "                'num_att': ['Rooms', 'Distance', 'Bathroom', 'Car', 'YearBuilt', 'Lattitude'], \n",
    "                'num_poli_att': ['Rooms', 'BuildingArea'], 'cat_att': ['Type', 'Regionname', 'Postcode'], \n",
    "                'cat2mean_att': ['Type', 'CouncilArea', 'Suburb', 'Regionname', 'Postcode']\n",
    "}\n",
    "\n",
    "final_full_pipe_lr = ColumnTransformer([\n",
    "            (\"num0\", rf_pipe_num0, atr_final_lr[\"num_log_att\"]),\n",
    "            (\"num1\", rf_pipe_num1, atr_final_lr[\"num_att\"]),\n",
    "            (\"poli\", rf_pipe_poli, atr_final_lr[\"num_poli_att\"]),\n",
    "            (\"cat\",  rf_cat_pipe, atr_final_lr[\"cat_att\"]),\n",
    "            (\"cat_2_mean\",rf_cat2mean_pipe, atr_final_lr[\"cat2mean_att\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Linear Regresor ----\n",
      "Mean: 365329.37419981335\n",
      "Standard deviation: 37261.00636141085\n"
     ]
    }
   ],
   "source": [
    "\n",
    "linear_regressor = LinearRegression()\n",
    "# Preparamos los datos con la pipe final\n",
    "X_train_prepared = actual_full_pipe.fit_transform(X_train_lr,y_train_lr)\n",
    "# Ejecutamos el modelo\n",
    "final_score = cross_val_score(linear_regressor, X_train_prepared, y_train_lr,scoring=\"neg_root_mean_squared_error\", cv=5, n_jobs=-1)\n",
    "display_scores(-final_score, \"Linear Regresor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf = X_train.copy()\n",
    "y_train_rf = y_train.copy()\n",
    "model = RandomForestRegressor(random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared:  (4345, 516)\n"
     ]
    }
   ],
   "source": [
    "# Esta será la pipeline para el modelo Random Forest\n",
    "\n",
    "#Para las columnas con muchos ceros sin sentido. A las que además se les aplicará la función logaritmo\n",
    "num0_pipeline = Pipeline([\n",
    "        ('zeros2NaN',FunctionTransformer(func = replace_0_2_NaN,validate=False)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('log',FunctionTransformer(np.log1p, validate=True)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "#Para las otras columnas numéricas\n",
    "num1_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "#Para las variables categóricas. Principalmente utilizaremos el OneHotEncoder\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"constant\",fill_value='Unknown')),\n",
    "        ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "\"\"\"\n",
    "Columnas numéricas:  ['Rooms', 'Price', 'Distance', 'Bedroom2', 'Bathroom', 'Car', \n",
    "'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "\n",
    "Columnas categóricas:  ['Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n",
    "\"\"\"\n",
    "\n",
    "#Estas son las columnas seleccionadas por mi para cada pipeline.\n",
    "num_attribs0 = ['Landsize','BuildingArea']\n",
    "num_attribs1 = ['Distance', 'Bedroom2', 'Bathroom', 'Car', \n",
    "'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
    "cat_attribs = [\"CouncilArea\",'Type','Suburb','Postcode']\n",
    "\n",
    "#Creamos la \"full_pipeline\", es decir la pipeline que engloba a todas las otras.\n",
    "full_pipeline_rf = ColumnTransformer([\n",
    "        (\"num0\", num0_pipeline, num_attribs0),\n",
    "        (\"num1\", num1_pipeline, num_attribs1),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "])\n",
    "\n",
    "#Preprocesamos los datos utilizando la \"full_pipeline\"\n",
    "X_train_prepared = full_pipeline_rf.fit_transform(X_train_rf,y_train_rf)\n",
    "print(\"Prepared: \",X_train_prepared.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trained model:\n",
      "RandomForestRegressor(max_features=110, n_estimators=82, random_state=100)\n",
      "Best parameters:\n",
      "{'max_depth': None, 'max_features': 110, 'n_estimators': 82}\n",
      "Best Score\n",
      "550.1148792382141 302626.380359275\n"
     ]
    }
   ],
   "source": [
    "rf_param_grid = {\n",
    "                'max_features': range(10,120,50),\n",
    "                'n_estimators': [82],\n",
    "                    'max_depth': [None],\n",
    "                   #'bootstrap': [False],\n",
    "                #\"min_samples_split\":[2,3],\n",
    "                #\"min_samples_leaf\":[1,2],\n",
    "}\n",
    "apply_gridsearch(model=RandomForestRegressor(random_state=100),\n",
    "                 params=rf_param_grid,\n",
    "                 X_train_gs=X_train_prepared,\n",
    "                 y_train_gs=y_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trained model:\n",
      "RandomForestRegressor(max_features=110, n_estimators=82, random_state=100)\n",
      "Best parameters:\n",
      "{'max_depth': None, 'max_features': 110, 'n_estimators': 82}\n",
      "Best Score\n",
      "550.1148792382141 302626.380359275\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(random_state=100)\n",
    "rf_param_grid = {\n",
    "                'max_features': range(10,120,50),\n",
    "                'n_estimators': [82],\n",
    "                    'max_depth': [None],\n",
    "                   #'bootstrap': [False],\n",
    "                #\"min_samples_split\":[2,3],\n",
    "                #\"min_samples_leaf\":[1,2],\n",
    "}\n",
    "rf_grid_search = GridSearchCV(model, param_grid=rf_param_grid, cv=5,\n",
    "                           scoring='neg_root_mean_squared_error',\n",
    "                           return_train_score=True,n_jobs=-1)\n",
    "rf_grid_search.fit(X_train_prepared, y_train)\n",
    "\n",
    "print(\"Best trained model:\")\n",
    "print(rf_grid_search.best_estimator_)\n",
    "print(\"Best parameters:\")\n",
    "print(rf_grid_search.best_params_)\n",
    "print(\"Best Score\")\n",
    "print(np.sqrt(-rf_grid_search.best_score_),-rf_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
